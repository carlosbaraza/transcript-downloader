# #143 - John Ioannidis, M.D., D.Sc.: Why most biomedical research is flawed, and how to improve it

**Channel:** Peter Attia MD
**Upload Date:** 2021-01-04
**URL:** https://www.youtube.com/watch?v=gzLANQ7xkD8
**Duration:** 112 minutes

## Description

John Ioannidis is a physician, scientist, writer, and a Stanford University professor who studies scientific research itself, a process known as meta-research. In this episode, John discusses his staggering finding that the majority of published research is actually incorrect. Using nutritional epidemiology as the poster child for irreproducible findings, John describes at length the factors that play into these false positive results and offers numerous insights into how science can course correct.

We discuss:
00:00:00 - Intro
00:02:40 - John’s background, and the synergy of mathematics, science, and medicine 
00:10:00 - Why most published research findings are false 
00:19:30 - The bending of data to reach ‘statistical significance’, and the how bias impacts results 
00:26:00 - The problem of power: How over- and under-powered studies lead to false positives 
00:31:00 - Contrasting nutritional epidemiology with genetics research
00:38:45 - How to improve nutritional epidemiology and get more answers on efficacy 
00:52:30 - How pre-existing beliefs impact science 
01:03:45 - The antidote to questionable research practices infected with bias and bad incentive structures 
01:12:00 - The different roles of public, private, and philanthropic sectors in funding high-risk research that asks the important questions 
01:21:30 - Case studies demonstrating the challenge of epidemiology and how even the best studies can have major flaws 
01:31:00 - Results of John’s study looking at the seroprevalence of SARS-CoV-2, and the resulting vitriol revealing the challenge of doing science in a hyper-politicized environment 
01:47:45 - John’s excitement about the future 

Show notes page: https://peterattiamd.com/johnioannidis/

About:
The Peter Attia Drive is a weekly, ultra-deep-dive podcast focusing on maximizing health, longevity, critical thinking…and a few other things. With over 30 million episodes downloaded, it features topics including fasting, ketosis, Alzheimer’s disease, cancer, mental health, and much more.

Peter is a physician focusing on the applied science of longevity. His practice deals extensively with nutritional interventions, exercise physiology, sleep physiology, emotional and mental health, and pharmacology to increase lifespan (delay the onset of chronic disease), while simultaneously improving healthspan (quality of life).

Learn more: https://peterattiamd.com/
Subscribe to receive exclusive subscriber-only content: https://peterattiamd.com/subscribe/
Sign up to receive Peter's email newsletter: https://peterattiamd.com/newsletter/

Connect with Peter on:
Facebook: http://bit.ly/PeterAttiaMDFB
Twitter: http://bit.ly/PeterAttiaMDTW
Instagram: http://bit.ly/PeterAttiaMDIG

Subscribe to The Drive:
Apple Podcast: http://bit.ly/TheDriveApplePodcasts
Overcast: http://bit.ly/TheDriveOvercast
Spotify: http://bit.ly/TheDriveSpotify
Google Play: http://bit.ly/TheDriveGooglePlay

## AI Summary

Here's my comprehensive analysis of this episode of The Drive podcast with John Ioannidis:

1. **Executive Summary**:
This episode features a discussion with John Ioannidis, a physician-scientist and Stanford professor known for his work in meta-research and scientific methodology. The conversation centers on the credibility of medical research, with particular focus on his seminal 2005 paper suggesting most published research findings are false. They discuss issues in nutritional epidemiology, research methodology, and the challenges faced during COVID-19 research.

2. **Key Medical/Scientific Points**:
- Most published research findings are likely to be false due to various biases and methodological issues [00:45:00]
- P-values of 0.05 are often inadequate for determining true significance [00:52:30]
- Underpowered studies can lead to both false negatives and false positives [00:55:00]
- The field of genetics has successfully implemented better research standards compared to nutritional epidemiology [01:15:00]

3. **Health Optimization Tips**:
Universal recommendations:
- Be skeptical of single nutritional studies showing dramatic effects [01:20:00]
- Consider Mediterranean diet as potentially beneficial, though evidence needs more validation [01:25:00]

4. **Notable Quotes**:
"If you don't fail, you're not going to succeed" - regarding scientific research [01:35:00]

5. **References & Resources**:
- 2005 PLoS Medicine paper on why most published research is false [00:45:00]
- PREDIMED Mediterranean diet study and its subsequent retraction/republication [01:25:00]
- Santa Clara COVID-19 seroprevalence study [01:40:00]

6. **Follow-up Questions**:
1. How can we improve the quality of nutritional research?
2. What specific reforms are needed in scientific publishing?
3. How can we better protect scientists from political attacks while maintaining scientific integrity?

Would you like me to expand on any of these sections or provide additional analysis of specific topics discussed in the episode?

## Transcript

[00:00:03] [Music] hey everyone welcome to the drive podcast i'm your host peter etia this podcast my website and my weekly newsletter all focus on the goal of translating the science of longevity into something accessible for everyone our goal is to provide the best content in health and wellness full stop and we've assembled a great team of analysts to make this happen if you enjoy this podcast we've created a membership program that brings you far more in-depth content if you want to take your knowledge of the space to the next level at the end of this episode i'll explain what those benefits are or if you want to learn more now head over to peteratiamd.com forward slash subscribe now without further delay here's today's episode [Music] my guest this week is john ionides john is by all estimates of polymath he's a physician scientist a writer and a stanford university professor he has extensive training in mathematics medicine epidemiology he's just generally one of the smartest people i've ever met and i've had the luxury of knowing john for probably about nine years and anytime i get to interact with him whether it's over a meal or more formally through various research collaborations it's just always an incredible pleasure john studies scientific research itself a process known as meta research primarily in clinical medicine but also somewhat in the social sciences he's one of the world's foremost experts on the credibility of medical research he's the co-director of the meta research innovation center at stanford in this episode we talk about a lot of things we talk about his uh his journey from greece to the united states but we talk a lot about some of his seminal papers you're going to see me reference a number of papers beginning with i think one of the most famous papers he's written although by citation it turns out to not be the most famous there's actually papers that even exceed it which is an amazing paper where he describes through a mathematical model why most published research in the biomedical field is incorrect which is obviously out of the gate a staggering statement we go on to discuss a number of his other seminal papers and then really kind of tackle some of the heart issues in medical research including my favorite topic nutritional epidemiology as always john is candid and full of insight so i'm just going to leave it at that and hope that you trust me and and make time to listen to this one so please without further delay enjoy my discussion with john iones [Music] john this is really exciting for me to be as close to sitting down with you as i can be during this time i've been wanting to interview you for as long as i've had a podcast and obviously we've known each other for probably close to 10 years now of course you first came on my radar in 2005 with a paper that we're going to spend a lot of time discussing today but before we get to that how would you describe yourself to people because you have such a unique background i i think that it's very difficult to know yourself and i've been struggling on that front for a long time so i'm trying to be a scientist i think that this is not an easy job it means that you need to reinvent yourself all the time you need to search for new frontiers for new questions for new ways to correct errors and to correct your previous self in some way so under that denominator of scientists in the works probably it would be a good place to put my whereabouts now your background is also in mathematics and i think that's part of my appreciation for you is the rigor with which you you bring mathematics to the study of science and in particular we're going to discuss some of your work and how you use mathematical models as tools to create frameworks around this now you were born in the u.s but grew up in greece is that correct indeed i was i was born in new york in new york city but i i grew up in athens and i always loved mathematics i think that mathematics are the foundation of so many things and they they can really transform our approach to questions that without mathematics it would be very difficult to make much progress how did you navigate your studies because you were obviously very prolific in mathematics if i recall reading somewhere in one of your bios you you even you know won the highest honor that a graduating college student could win in mathematics in greece at the time how did you decide to also pursue something in the biological sciences in parallel as opposed to staying purely in the natural or philosophical sciences of mathematics medicine had the attraction of being a profession where you can save lives and i think that intellectual curiosity is very interesting but the ability to make a difference for human beings and to save lives to improve their quality of life seemed to be at least in my eyes as a young person something that was worthwhile pursuing i had a very hard time to choose what pieces of mathematics and science and and medicine i could combine in what i wanted to do i think that i have tried my hands in very different things i have probably failed in all of them but in some ways i saw that these were complementary so i believe that medicine is amazing in terms of its possibilities to to help people you need however very rigorous science you need very rigorous scientific method to be applied if you want to get reliable evidence then you also need quantitative approaches you need quantitative tools to be able to do that so i think i think that none of them is possible to dispense without really losing the whole and and losing the opportunity to do something that that really matters eventually and your parents were physicians as well is that correct indeed both of them were physicians actually physician scientists so i did have an early exposure to an environment where i could hear their stories of clinical exposure at the same time i could see them working on their research i remember these big tables with scientific papers spread all over them and with what were the early versions of computerized research i think that i had the chance to be exposed to the software and computers in an early phase because my my father and my my parents were interested in doing research so you finished medical school and your postgraduate training also in greece or did you do part of that in the united states i finished medical school in greece in athens in the the national university of athens and then i went to harvard for residency training and then tufts jungle medical center for training and infectious diseases at the same time i was also doing joint training in healthcare research so it was very interesting and fascinating years learning from great people and who were some of the the people that you think back as having kind of shaped your thinking during those years in the medical school i i had some great teachers one of them was the professor of epidemiology dimitri drakopoulos who was also chair of epidemiology at harvard and he had some really great statisticians in his team so from the the first year at medical school i went to meet them and tried to use every textbook that they could give me and every resource that i could play with in my residency training i was very fortunate to meet a great physician scientist especially in infectious diseases actually bob malloring was the physician-in-chief and a professor at harvard professor of medical research as well and he was really an amazing personality in terms of his clinical acumen and his approach to patients also his very temperate mode of uh dealing with very serious problems and dissecting through the evidence in trying to make decisions and of course start with making diagnosis at the end of my residency training i had the pleasure to meet the late tom chalmers along with joe lau they were at tufts at that time and my meeting with them was really a revelation because they were the ones who were advancing the frontiers of evidence-based medicine evidence-based medicine had just been coined as a term pretty much by the mcmaster team david sackett and gordon wyatt and tom chalmers was the first person in the us to design a randomized trial he was also one of the first to perform meta-analysis that had a major impact in medical science at the time that i met them they had just published an influential paper on cumulative meta-analysis in the new england general medicine and it was a revelation for me because somehow what they were proposing was mixing mathematics rigorous methods evidence and medicine in one coherent hall which seemed to be a forlorn hope until then for me i was just seeing lots of clinical exposures that there was very little evidence to guide us there was no data or very poor data and a lot of expert based opinion guiding everything that was being done and so this is just temporally i mean chalmers died in the mid 90s so this is what the early 90s that you were fortunate enough to meet him yes i met him in 1992 and he died about five years later i was grateful that i had the opportunity to work with him and also with joseph lau who was at that time at tufts junior medical center which i went eventually to do my fellowship training because there are so many things i want to talk about john and we don't have the luxury of spending 12 hours together i'm going to fast forward about a decade i'm going to fast forward to 2005 to that paper that i alluded to at the outset which was the first time your work came on to my radar which is not to say anything other than that's just the first time i became aware of sort of the gravity of your thinking can you talk a little bit about that it was in plos one was that paper correct yes it was in place medicine plus medicine okay so this is basically an open source journal that i think another stanford professor actually was one of the guys behind this journal if i recall pat brown was one of the forces behind plos correct well it was a transformative move at that time trying to create a new standard for medical journals i think that now this has become very widespread in in a way but i think back then it was something new something that it was a new frontier in a sense so you wrote a paper that on the surface seems i mean highly provocative right the title of the paper is something to the effect of why most published clinical research is untrue i mean that's the gist of it can you walk people through the methodology of this it's a theoretical paper but explain to people who maybe don't have the understanding of mathematics that you do how you were able to come to such a stark conclusion which i want to point out one thing i'll give you why i had an easy time believing the results of your paper is my mentor had shared with me a statistic when i was you know sort of doing my postdoctoral training which i found hard to believe but when i realized it was true became the book end to your claim and that was some at the time something to the tune of 70 of published papers were never cited again outside of auto citation meaning outside of the author citing his or her own work and if you think about that for a moment if 70 of work can't even be cited by one additional person down the line that tells you it's you know either irrelevant or wrong so again that's not the same thing that you said but it at least primed me to kind of listen to the message you were you were talking about so talk a little bit about that paper that paper as you say it's a mathematical model that is trying to match empirical data that had accumulated over time both in my work and also in the work of many other scientists who were interested to understand the validity of different pieces of research that was being produced i think that many of us had been disillusioned that when evidence-based medicine started we thought that now we have some tool to be able to get very reliable evidence for decision making and very quickly we realized that biases and results that could not be replicated and results that were overturned and results that were unreliable were the vast majority it was not something uncommon it was the rule that we had either unreliable evidence or actually perhaps even more commonly no evidence so it's an effort that paper to put a mathematical construct together that would try to explain what is going on and would also try to predict in some ways what might happen in if some of the circumstances would change in terms of how we do research so the model makes for a framework that is trying to calculate what is the chance that if you come up with a eureka a statistically significant result that you claim i have found something i have found some effect that is not null there is some treatment effect here there's some not zero that i'm talking about what are the chances that this is indeed a non-null effect that we're not seeing just a red herring and in order to calculate the chances that this is not just a red herring you you need to take into account what is your prior chances that you might be finding something in the field that you're working there are some fields that probably have a higher chance of making discoveries compared to others if you're unlucky to work in a field that there's nothing to be discovered you may be wasting your time and publishing one million papers but you know there's nothing to be discovered so it's going to be one million papers that end up with nothing conversely there may be other fields that may be more rich in discovery both the field and the tools the methods that and the designs of the studies that we throw at trying to answer these questions can be informative the second component is in what environment of power are we operating meaning is the study large enough to be able to detect non-null effects of some size of interest or maybe there are true effects out there but our studies are very small and therefore they're not able to detect these effects and in my experience until that time i had seen again and again lots of very small studies floating around with results that were very questionable that could not be matched with other efforts especially when we were doing larger studies most of them seem to go away and power is important not only because if you don't have enough power you cannot detect things that exist what is equally bad or probably worse is that if you operate in an environment of low power when you do get something detected it is likely to be false and here comes the other factor that is compounding the situation bias which means that you have some results that for whatever reason bias makes them to seem statistically significant while they should not be and bias could take zillions of forms i think that throughout my career i feel like i'm struggling with bias with my own biases and with biases that i see in in the literature but bias means that you could have conscious unconscious or subconscious reasons why a result that should have been null somehow is transformed into a significant signal it could be publication bias it could be selective reporting bias it could be multiple types of confounding bias it could be information bias it could be many many other things that turn null results into seemingly significant results while they are not then you have to take into account the universe of the scientific workforce where we're not talking about a single scientist running all the studies it's not just a single scientist or a single team we have currently about 35 million people who have co-authored at least one scientific paper we have many many scientists who might be trying to attack the same scientific question and each one of them is contributing to that evidence however there's an interplay of all these biases with all of these scientists so if you take into account that multi-scientist environment multi-effort environment you need to account for that in your calculations because if you for example say what are the chances that at least one of these scientists will find some significant signal this is a very different situation compared to just having one person taking a shot and just taking a single shot so this is pretty much what the model tried to take into account putting these factors together and then trying to see what you get under realistic circumstances for these factors these factors would vary from one field to another they would be different for example if we're talking about exploratory research with observational data versus small randomized trials versus very large phase three or even mega trials it would be different if we're talking about massive testing like what we do in in genetics versus highly focused testing of just one highly specified pre-registered hypothesis that is being attacked running the calculations the model shows that in most circumstances where both biomedical research but i would say most other fields of research are operating if you get a nominally statistically significant signal with a traditional p-value of slightly less than 0.05 then the chances that you have a red herring that you know this is not true that it is a false positive are higher than 50 there is a huge gradient and in some cases it may be much lower the false positive rate may be much much lower but and in others it would be much higher but but in in most circumstances the chances that you got it wrong are pretty high they're very high that's actually a very elegant description of that paper i want to go back and unpack a few things for people who maybe don't have some of the acumen down so let's go a bit deeper into what a p-value is everybody hears about it and everybody hears the term statistically significant so maybe explain what a p-value is explain statistical significance and explain why it's not necessarily the same as clinical significance and why we shouldn't confuse them i think that there's major misconceptions around significance what we care in medicine is clinical significance meaning if i do something or if i don't do something would that make a difference to my patient or it could be in public health to the community to cohorts of people to healthy people who want to have preventive measures and so forth do i make a difference does it matter is it big enough that is worthwhile the cost the potential harms the implementation effort perhaps other alternatives that i have how does that compare to these alternatives maybe they're better or cheaper or easier to implement or have fewer harms so this is really what we want to answer but unfortunately most of the time we are stuck with uh trying to answer very plain frequentist approach question which boils down to statistical significance typically this boils down to a p-value threshold of 0.05 for most scientific fields over the years there's many scientific fields that have diversified and they have asked for more stringent levels of statistical significance a couple of years ago along with many other people we suggested that fields that have not diversified and they do not adjust their levels of statistical significance to more stringency by default they should be using a more stringent threshold for example use a threshold of 0.005 instead of 0.05 however most scientists are trained with statistics light to use some statistical test that gives you some statistic that eventually translates to a p-value and what that p-value means it needs to be interpreted as what are the chances that if i had an infinite number of of studies like this one i would get a result that would be as extreme or or more extreme and even that is not a complete definition because it does not take into account bias because maybe you would get a result that is as extreme but it's largely because of bias for example there's many many fields that you can easily get p-values that are astronomical they're not just less than 0.005 but they may be 10 to the minus 100 with some of the large databases that we have we can easily get to astronomically small p-values but this doesn't mean much it could just mean that you have bias and this is why you get all these astronomically low p-values but they don't really mean that the chance of getting such an extreme result is is extremely implausible and that there's something there it just means that certainly there's bias no more than that there has been what i call the statistics wars over the last several decades people have tried to diminish the emphasis on statistical significance i think i have been in the camp of those who have argued that we should diminish emphasis or at least try to improve the understanding of what that means for people who use and interpret these p values in the last few years this has become probably more aggressive many great methodologies have suggested that we should completely abandon statistical significance that we should just ban the term never use it again and just focus on effect sizes focus on how much uncertainty we have about effect sizes focus on perhaps basic interpretation of research i have been a little bit reluctant about adopting the ban statistical significance approach because i i'm afraid that we have all these millions of scientists who are probably not very properly trained to understand statistical significance but they're completely not trained at all to understand anything else that would replace it so in some ways for some types of design so i would argue that if you pre-specify and if you are very careful in registering your hypotheses and you have a protocol that you deposit for example what is happening or should be happening with randomized trials and you have worked through this that it makes sense that your hypothesis is clinically important that the effect size that you're trying to pick is clinically meaningful it is clinically significant then i would argue that statistical significance and using a p value threshold whatever that is depending on how you design the study makes perfect sense it's actually a very transparent way of having some rules of the game that then you try to see whether you manage to succeed or not so if if you remove these rules of the game after the fact in these situations it may make things worse because you will have a situation where people will just get some results and then they will be completely open to interpret them as they wish and we see that they interpret them as they wish even now without any rules in the game or at least by removing those rules post post-hoc but if we could have some rules for some types of research i think that this is useful for other types of research i'm willing to promote better ways of interpreting results but this is not going to happen overnight we have to take for granted that that most scientists are not really well trained in statistics and they will misuse and misinterpret and misapply statistics unfortunately so we need to find ways that we will minimize the harm we will minimize the error and maximize in medicine the clinically significant pieces and in other sciences the true components of the research enterprise now at the other side of that statistical field is power right so we go from alpha to beta and you alluded to it earlier i want to come back to it because you actually said something very interesting i think most people who dabble enough in the literature understand that if you underpower a study so if you have too few samples too few subjects whatever the case might be and you fail to reach statistical significance it's not clear that you failed to reach statistical significance because you should be rejecting the null hypothesis or because you didn't have a large enough sample size so that's always the fear right the fear is that you get a false negative but you said something else that i thought was very interesting if i heard you correctly which was no you actually run the risk of a false positive as well if you're underpowered can you say more about that indeed in in an underpowered environment you run the risk of having higher rates of false positives if you take the performance of the field at large you know if you take hundreds and thousands of studies that are done in an underpowered environment even if you manage to detect the real signals you know signals that that do exist if these signals are detected in an underpowered environment their estimates will be exaggerated compared to what the true magnitude is and in in many situations both in medicine and in other sciences it's not important so much to find whether there's some signal at all which is what a null hypothesis is trying to to work around but how big is the signal i mean if a treatment has a minuscule benefit then i wouldn't care about it i i wouldn't use it because the cost and the harms and and everything on the other side of the balance is is not making it worth it so most scientific fields have been operating in underpowered environments and and there's many reasons for that and it varies a little bit from one field to another but there are some common denominators number one we have a very large number of scientists scientists are competitive there's very limited resources for science it means that each one of us can get a very thin slice of resources we need to prove that we can get significant results so as to continue to be funded and to be able to advance in in our career this means that we are stuck in a situation where we need to promote seemingly statistically significant results even if they are not we need to do very small studies with these limited resources and and then do even more small studies rather than aim to do it a more definitive large study there's even a disincentive towards refuting results that are not correct because that means that you feel that you're back to square zero you cannot make a claim for continuing your funding all the incentives at least until recently have been aligned towards performing small studies in very selectively reported circumstances and with flexibility in the way that results are analyzed and presented and i think that this leads to very high rates of results that are either completely false positives or they may be pointing to some real signal but but the estimate of the magnitude of the signal is grossly exaggerated in recent years we have started seeing the opposite phenomenon as well we start seeing some fields that have overpowered studies instead of just having very small studies in some fields we have big data which means that you can access records medical records from electronic health records on millions of people or you may have genetic information that is highly granular and and gives you tons of information and big data are creating an opposite problem it means that you're overpowered and you can get statistically significant results that have no clinical meaning that have no meaning really and and even with a tiny little bit of bias you may get all these signals just because bias is there so you're just measuring bias you're just getting a big scale assessment of the distribution of bias in your data sets that's becoming more of a problem in some specific fields i think that the growth of this type of problem will be faster compared to the growth of the problem of small underpowered studies i think in most fields it's a more common problem though until now that we have very small studies rather than very large studies now you've commented on was studies do you want to do you want to talk a little bit about that here it sort of fits into this a little bit doesn't it genetics was something that i was very interested in from my early years of doing research because it was a new frontier for quantitative approaches lots of very interesting methodology was being developed in genetics many of the questions of evidence that had been stagnating in other biomedical fields they had a new opportunity to give us some new insights with much larger scale evidence in genetics compared to what we had in the past when we were trying to measure things one at a time especially genetics was a fire hose of evidence in in some way so i found it very exciting and for many years i did a lot of genetic research i still do some and very early on we realized through genetics that the approach that we had been following in most traditional epidemiology like looking at one risk factor at a time and trying to see whether it is associated with some disease outcome was not getting very far we could see in genetic epidemiology of candid genes that most of these papers that we're looking at one or a few genes at a time with association with some outcome just trying to cross the threshold of statistical significance and then claiming success they were just false positives we saw that pretty early it took some time for people to be convinced but then they were convinced and genetics took some steps to remedy this they decided to do very large studies to start with they also decided to look at the entire genome look at all the factors rather than one at a time and they also decided to join forces not have each scientist try to publish their results alone but share everything have a common protocol put all the data together to maximize power to maximize standardization to maximize transparency also and then report the cumulative results from the combined data from all the teams that had contributed to these large meta-analysis of primary data so this is a recipe that i think should be followed by many other fields especially fields that work with observational data in epidemiology and some fields have started moving in that direction as well but not necessarily as much as the revolution that happened in genetics and population genomics so i was going to actually ask you exactly that question i was going to save it for a bit later but let's do it now why did the field of genetics basically have the ability to self-police and undergo this cultural shift in a way that let's just put every card on the table here nutritional epidemiology has not been nutritional epidemiology which we're going to spend a lot of time talking about is the antithesis of that and it continues to propagate subpar information which is probably the kindest thing i could say about it so what is it culturally about these two fields that has produced such stark contrasts in the response to a crisis there's multiple factors one reason is that genetics managed to have better tools for measurement compared to nutritional epidemiology we managed to decode the human genome so we developed platforms that could measure the entire variability more or less in the human genome with pretty high accuracy if you have genotypic platforms that have less than 0.01 error rate this means that you have very accurate measurement as opposed to nutrition where the traditional tools have been questionnaires or survey tools that have very high biases very high recall bias very low accuracy and they do not really capture the diversity of nutritional factors with equal granularity as we can capture the the genetics in their totality of the human genome the second reason was that i believe in genetics there were no strong priors no strong beliefs no strong opinions no strong experts who would fight with their lives for one gene variant versus another we had some you know i think that some of us probably might have published about one gene and then we would fiercely defend it because obviously if you publish a paper you don't want to be proven wrong i think it's it's very human but it was nothing compared to the scale that you see in nutrition research where you have a very strong expert opinion base of people who have created careers and they feel very strongly that this type of diet is saving lives and it should have policy implications it should change the world it should change our guidelines should change everything many of these beliefs are interspersed with religious or cultural or you know non-scientific beliefs in shaping what we think is good diet and as you realize none of that really exists for genetics you know polymorphism rs 2492-14 is is unlikely to be endorsed by any religious cultural political or dietary proponents it's a very different beast and i think that you can be more neutral with genetics research because of this objectivity as opposed to nutrition where there's a lot of heavy beliefs interspersed methodologically also genetics advanced faster nutrition has been stuck mostly in the era of using p-values of 0.05 thresholds and using those thresholds in mostly post-hoc research research that is not registered that is selectively presented people are trained in a way that they need to play with the data they need to torture the data they need to try to unearth interesting associations and in some cases of course this becomes extreme like what we have seen in the cornell case where you know pretty much goes into the situation where you have fraud i mean it's not just poorly done research it's it's fraudulent research but fraudulent research aside even research that is not fraudulent in nutrition has some standards of methods that are pretty suboptimal compared to what genetics has adopted that they decided that we have such a huge multiplicity that we need to account for that so you know we're not going to claim success for a p-value of 0.05 we will claim success for a p-value of 10 to the minus 8. and if it's not that low then forget it that's not really a finding we need to get more data before we can we can say whether we have a finding or not or they decided that they will share data that they will create large coalitions of researchers who would all share their data they would standardize their data they will standardize the analysis they would perform analysis in a very specific way and they would also sometimes actually i think this is becoming the norm have two or three analyst teams analyze the same data and make sure that they get the same results these principles and these practices have started being used in fields like nutrition but but to a much lesser extent and i think that gradually we will see more of that but it's going to take some time so there's multiple scientific and behavioral and cultural and statistical and methodological reasons why these fields have not progressed at the same pace of revolutionizing their research practices let's talk a little bit about austin bradford hill i'm guessing you didn't have a chance to meet him he died in 91 would you have crossed paths with him at all no i didn't have that fortune unfortunately do you think he would be rolling around in his grave right now if he saw what was being employed based on the criteria he set forth which i also want to talk about your thoughts around the revision of these but even if you just take his 10 criteria which we'll go through for a moment as a bit of a background on epidemiology do you think that what he had in mind is what we're doing today i think that austin bradford hill was very thoughtful he was one of the fathers of epidemiology and of course he didn't have the the measurement tools and the capacity to run research and starts large scale as we did today but he was spot on in coming up with good questions and asking the right questions asking the important questions so his criteria i don't think that he he thought of them as criteria and and i i don't think that he ever believed that they should be applied as a hard rule to arbitrate that we have found something that is causal versus something that is not causal if if you read through the paper it's a classic it's very obvious that he has a very temperate approach he has a very cautious approach basically he says none of these items is really bulletproof i can always come up with an example where it doesn't work and i think that this is really telling what a great scientist he was because indeed in science there's hardly anything that is bulletproof i i don't know you know the laws of gravity might be bulletproof but even those as you realize they're just uh only down to atomic levels yeah exactly you know in in the theory of relativity they would start failing so he was very cautious i think that paper had tremendous impact i think that we have not been very cautious in moving forward with many of our observational associations and the claims that we have made about them i don't want to give any holistic perspective and i don't want to give let's say a very negative perspective of epidemiology because we run the risk of entering the other side where you will have some signs deniers saying so you're not certain and therefore we can have more air pollution you know we can have more pesticides we can have more that's that's not clearly the case i mean we we have very solid evidence for many observational associations there's not the slightest doubt that tobacco is is killing right and left it it's likely to kill one billion let's let's go through let's go through tobacco as the poster child for bradford hill's criteria so i'm going to rattle off the quote-unquote criteria and just use tobacco as a way to explain it so let's let's start with strength how does the association between tobacco and lung cancer fit in terms of causality vis-a-vis this criteria of strength it is huge i mean we do not see odd ratios of 10 20 and 30 as we see with tobacco with many types of cancer and with other outcomes like cardiovascular disease and i think that that really stands out and and we see that again and again and again we see very strong signal we see signals that are highly replicable and that's the exception in most of what we do nowadays in epidemiology we don't see odd ratios of 20. if i see an odds ratio of 20 in my calculations i'm almost certain that i have something wrong i always go back and check and i find an error that i have done and so you're probably off by a log if you're getting a 20 nowadays probably too long i i think in in genetics we are dealing actually with odds ratios of 1.01 at the time so yeah so 1.01 may still be real and of course you know then the question is is it clinically relevant yeah it's unlikely to be clinically relevant but you know how much certainty can you get even for its presence so the strength is huge you really essentially covered the next one which is consistency if you look at all of the studies in the 1950s and the 1960s they were all really moving in the same direction and that's whether you looked at physicians who were smokers non-physicians who were smokers whichever series of data you looked at you basically saw this 10x multiplier in smoking and i think on average it worked out to be about 14x there was about a 14 times higher chance i mean that's a staggering number what about specificity what is what does specificity refer to here i think that if you have such strength and such consistency i i would probably not worry that much about the rest of the criteria you know i think that criteria like specificity or like analogy they're far more soft in terms of what they would convey and and also we just don't know the the nature of nature in how it operates many phenomena may be very specific but it doesn't have to be so we should not take it for granted that we should see perfect specificity or low specificity we see many situations where where you have multi-dimensional situations of causality you have multiple factors affecting some outcome or you have one factor affecting multiple outcomes the density of the webs of causality can be highly unpredictable so i would i would not worry that much about other criteria if if you have some like strength and consistency being so impressive in these cases now in most cases we don't have that right we'll get an odds ratio of 1.14 which of course is a 14 relative increase as opposed to uh you know 14x so in those situations when strength and consistency are out the window which is essentially true of everything in nutritional epidemiology i can't really think of examples in nutritional epi where you have strength and consistency well major deficiencies i think would belong to the category of very clear signals major nutritional deficiencies you know if you have like yes yeah yeah very very for example sure sure yeah you're you know thymine deficiency where you're out to lunch yeah but do you then look at i mean even biological gradient gets very difficult with the tools of nutritional epi do you start to look at experiment plausibilities to me has always struck me as a very dangerous one because i don't know it just it seems a bit of hand waving i mean where do you where do you then look i think the first question is whether you can get experimental evidence to me that's the priority and i realized that in in some circumstances when you know that you're dealing with highly likely harmful factors you cannot really have equipoise to to do randomized trials but for most situations in nutrition to take nutrition as the example that we have been discussing you can do randomized trials and actually we have done randomized trials it's not that we're not doing randomized trials we have done many thousands of randomized trials most of them unfortunately are pretty small and underpowered and they suffer from all the problems that we discussed earlier with underpowered studies that are selectively reported with no preregistration and with kind of haphazardly done analysis and reporting i mean they're not necessarily better than observational data that suffer from the same problems but we also have a substantial number of very large randomized trials in nutrition we have over 200 large randomized trials most of those focus on specific nutrients or supplementations some are looking at diets like mediterranean diet and with very few exceptions they do not really show the benefits that were suspected or were proposed in the observational data there are exceptions but but there are not that many that to me suggests that most likely the interpretation that most of the observational signals are false positives or or substantially exaggerated is likely to be true we shouldn't be throwing out the baby with a bath water there may be some that are worth pursuing and that may be true and i think that this means that we need to do more trials the counter argument would be that well in a randomized trial especially a large one especially with long-term follow-up people will not adhere to what you tell them to do with their diet or nutrient intake or supplementation my response to this is that when it comes to getting evidence about what people should eat that lack of adherence is part of the game it's part of real life so if a specific diet is in theory better than another but people cannot adhere to that it's not really better because people cannot use it so i get the answer to the question that i that i'm interested in which is is that something that will make a difference of course it does not prove that biochemically or in a perfect system or in you know the perfect human who is uh uh eating like a robot that would not be helpful but but i don't care about treating robots i care about managing and helping real people i agree with that completely john i would throw in one wrench to that which is in a world of so much ambiguity and misinformation i do think it's important to separate efficacy from effectiveness what you're of course saying is in the real world only effectiveness matters so real world scenarios with real world people but i still think there is a time and a place for efficacy we do have to know what is the optimal treatment under perfect circumstances if we want to have any chance at for example informing policy i'll give you an example food stamps should food stamps preferentially target the use of certain foods over others well again if you had really efficacious data saying this type of food is worse than that type of food you could steer people towards healthier foods it could impact the way we subsidize certain foods in other words it's really all about changing the food environment so it's it is very hard to follow i think any diet is that is not the standard american diet so anytime you opt out of the standard american diet whether it be into a mediterranean diet or a vegetarian diet or a low carbohydrate diet or basically anything that's not the crap that we're surrounded by requires an enormous effort and i think a big part of that is because there is still so much ambiguity around what the optimal nutritional strategies are we haven't answered the efficacy question because i think we keep trying to answer the effectiveness question i agree and i think i would not abandon efforts to get some insights on efficacy but we're not really getting these insights the way that we have been doing things if i i think that if you want to get answers on efficacy there are options one is through the experimental approach so you can still run randomized trials but you can do them under very controlled supervised circumstances that you know people are in a physiology or metabolism clinic that they're being followed very stringently on what they eat and what happens to them and you can measure very carefully these biochemical and physiological responses i think that a second approach in the observational world or or between the observational and the randomized is mendelian randomization studies with the advent of genetics we have lots of genetic instruments that may be used to create designs that are fairly equivalent to a randomized design so you can get some estimates that are not perfect because mendelian randomization has its own assumptions and sometimes these are violated but at least i think that they go a step forward in terms of the credibility of the signals that you get and then you have the pure observational evidence which i don't want us to discard it completely i i think that these are data we should need to use them we just need to interpret them very cautiously if we use some of the machinery that we have learned to deploy in other fields for example one approach is what i call the environment-wide or exposure-wide association testing instead of testing and reporting on one nutrient at a time you just run an analysis of all the nutrients that you have collected information on and you can also do it for all the outcomes that you have collected information on so that would be an exposure outcome-wide association study and then you report the results taking into account the multiplicity and also the correlation structure between all these different exposures and outcomes you get a far more transparent and complete picture and if you get signals that seem to be recurrent and replicable across multiple data sets multiple cohorts that you run these analysis you start having higher chances of these signals to be reflecting some reality still it's not going to be perfect because of all the problems that we mentioned but but it is better compared to what we do now where we just go after finding yet one more association one at a time and coming up with yet another paper that is likely to be very low credibility john if your 2005 paper on the frequency with which we were going to come across valid scientific publications is arguably the one that's is that is that your most cited paper no it's not the most highly cited it's it's received i think close to 10 000 citations but for example the prisma statement for meta-analysis has received far more okay well if that i was going to assume that the 2005 paper was the most cited but i was going to say the most entertaining is your 2012 paper which is the systematic cookbook review and again this is just one of those things where i remember the moment this paper came out and just the absolute belly laughing that i had reading this and frankly the sadness i had reading this because it is a sarcastic commentary in a way on a problem that i think plagues this entire field so in this paper you basically i don't know if it was randomly but you selected basically 50 common ingredients from a cookbook right was this did was there any method behind how you did this or was it purely random well uh we used the boston cookbook that has been published since the 19th century and we randomly chose ingredients by selecting pages and and then within those the the recipes and the ingredients that were in these recipes so yes it is 50 ingredients a random choice thereof and trying to map how many of those have had published studies in the scientific literature in terms of their association with cancer risk and not surprisingly almost all of them had some public studies associating them with cancer risk even the exceptions were probably exceptions because of the way that we searched for example we didn't find any study on on vanilla but there were studies on vanillin so if we had changed with a if we had screened with the names of the biochemical constituents of these ingredients probably i guess all of them might have had some studies associating them with cancer risk how was this paper received by the nutritional epidemiology community i think it created lots of enemies and lots of friends and i'm grateful for the enemies who some of them have pushed back with constructive comments i i think that most people realize that we have a problem i think that even people who disagree with me on nutrition i have great respect for them and i'm sure that they're well intentioned i think that at the bottom of their heart it's not that they want to do harm they want to save lives they want to improve nutrition they want to improve our world so i think that it should be feasible to reach some synthesis of these different approaches and these different trends and i i do see that even people who have used traditional methods do start using some of the methods that we have proposed for example these exposure-wide approaches or or trying to come up with with large consortia and meta-analysis of multiple cohorts to strengthen the results and the standardization of the results i worry a little bit about some of the transparency of these efforts to give you one example i have always argued that if you can have large-scale meta-analysis of multiple teams ideally all the teams joining forces and publishing a common analysis with common standards and ideally these would be the best standards and and the best statistical tools thrown at the analysis this is much better than having fragmented publications so in in some questions of nutrition i have seen that happen but but here's what what goes wrong the invitation goes to other investigators who have already found results that square with the beliefs of the inviting investigator so there may be 3 000 teams out there and the invitation goes to the 100 teams that have claimed and believe that there is that association and then these data are cleaned combined and analyzed in in the way that has found the significant association already and you have a conclusion with an astronomically low p-value that here it is we have concluded that our claim for a significant association is indeed true and here's a large meta-analysis now this is equally misleading or even more misleading than the single studies because you have cherry-picked studies based on what you already know to be the case and putting them together you just magnify the cherry picking you you just solidify the cherry picking so one has to be very cautious magnitude and amount of evidence alone does not make things better actually it can make things worse you need to ask what is the foundational construct of how that evidence has been generated and identified and synthesized and in in some cases it may be worse than the single small studies that are fragmented because some of them may not be affected by the same biases there also seem to be sort of institutional issues around this right i mean your alma mater has a very strong point of view on nutritional epidemiology right i think this is unavoidable there are schools of thought in any scientific field and harvard has an amazing team of nutritional epidemiologists i have great respect for them even though probably we do not agree on on many issues i think that we should look beyond let's say the personal differences or or opinion differences i i i think that my opinion has less weight than anyone else's weight in that regard if i want to be true to to my standards i i'm not trying to promote something because it is an opinion what i'm arguing is for better data for better evidence for better synthesis and more unbiased steps in generating the evidence synthesizing the evidence and interpreting it and i'm willing to see whatever result emerges by that process i'm not committed to any particular result i would be extremely happy if we do these steps and we come up with a conclusion that oh 99 of the nutritional associations that were proposed were actually correct i have absolutely no problem with that if we do it the right way what i'm worried is resistance to doing it the right way i think your point earlier though about the difference between say how the genetics community and the nutrition community were able to sort of approach this problem i don't think you can forget your second point right which is it's very difficult to overcome prior beliefs and when an individual has made an entire career of a set of beliefs i think it requires a very special person to be able to say you know that may have been incorrect and that is independent of what that belief is by the way that can be a belief that may be correct or or maybe fundamentally incorrect you know it's funny i recently saw this thing on netflix it was the uh kind of documentary about this db cooper case do you remember do you know this db cooper case it's the only unsolved act of u.s aviation crime that's never been solved so did you know this case john that guy who hijacked an airplane and then jumped out the back in 1971 oh i may have heard of it somewhere but yeah i don't i don't recall it very well well it's interesting in that this guy hijacks an airplane with a bomb and requests that it the plane be landed while they pick up two hundred thousand dollars and four parachutes he then gets the plane to take back off and jumps out the back with the money and he's never been found nine years later they found a little bit of the money that's the only real clue and this documentary focused on four suspects four of many suspects and you basically hear the story of each of the four suspects and each of the people who today are making the case for why it was their uncle or their husband or whatever and my wife and i are watching this and we're thinking it's interesting and at the end i just said to her i said you know this is a great sort of example of human nature which is i believe every one of those people truly believes that it was their relative or friend or whomever who was db cooper and yet i think all of them are wrong i i think each of those four suspects is categorically not the person and yet each of them i am convinced by their sincerity and i i think that's the problem is i don't think science should be able to be that way that's the problem i think i have with epidemiology is that i guess i'm just not convinced it's a science and the way that we talk about science well we we have to be cautious because we are human and scientists have beliefs and i think that there's nothing wrong with having beliefs i think the issue is can we map these beliefs can we be transparent can we be as much restrained about how these beliefs are influencing the the contact of our research and the way that we interpret our findings it will never be perfect we are not perfect and and i think that aiming to be perfect is not tenable but at a minimum we should try to impose as many safeguards in the process just to minimize the chances that we will fool ourselves you know not not fool others but fool ourselves to start with us as feynman would would say this is not easy in fields that have a very deeply entrenched belief system and i think nutrition is one such again it there's no bad intention here people are well intentioned they they want to do good i will open a parenthesis of course there is some bad intentions there's big food there's industry who wants to promote their products and sell whatever they produce and you know that's a different story and it is another a huge confounder both in nutrition and in other fields that we have very high penetrance of financial conflicts but i think that non-financial conflicts and can also be important and at a minimum we should try to be transparent about them try to communicate both to the external world but but also to our own selves what might be our non-financial conflicts and and beliefs in in starting to go down a specific path of investigation and specific interpretation of uh results you referred to it very very briefly earlier what were the exact details of the case of brian wansick at cornell that was a lot to do and i it seemed that that went one step further that seemed like there was something quite deliberate going on well in that case it was revealed based on the communication of that professor with his students that practically he was urging them to cut corners and to torture the data until they would get some nice looking result and practically he was packaging nice looking results as soon as they would become available based on that data torturing process so the data torturing was the central force in in generating these dozens of papers that were creating a lot of interest and probably they were very influential many of them in terms of decision making but if you create results and significance in that fashion obviously the chances that these would be reproducible results is is very very limited yeah and of course he was a very prominent person in the field it makes you wonder how often is this going on with someone maybe less prominent where you know they're part of that 35 million people who are out there authoring the what are we about a hundred thousand papers a month make their way onto pubmed i mean it's an avalanche right we have a huge production of scientific papers as you say and if you look across all sciences probably we're talking about easily five million papers added every year and the number is accelerating every single year of course very few of them are both valid and useful and it's very difficult to sort through all that mountain of published information i think that research practices are substandard in in most scientific fields for most of the research being done there's a number of surveys that have been conducted asking whether fraud is happening and whether suboptimal research practices are being applied the results are different depending on whether you ask the person being interviewed on whether they are doing this or whether people in their immediate environment are doing this so fraud i think is uncommon i don't think that fraud is a common thing in science it does happen now and then but i don't think that it is a major threat in terms of the frequency it is a threat in terms of the visibility that it gets and the damage that it gets to to the reputation of science as an enterprise but but it's not common what is extremely common is questionable research practices or harmful research practices which means cutting corners in different ways and depending on how exactly you define that the percentage of people who might be cutting corners at some point is extremely high it may be approaching even 100 if you define it very broadly and if you include situations where people are not really cognizant about the damage that they do or the suboptimal character of the approach that they're taking and how it subverts the results and or the conclusions of the study now how do you deal with that do you deal with that with putting people away to jail or making them lose their jobs or making them pay one million dollars fines i don't think that that would work because you would probably need to fire the vast majority of the scientific workforce and all of these are good people you know they're not there because they're they're frauds but you need to work through training through sensitizing the community having a grassroots movement about realizing what the problems are how you can avoid these traps and how you can use better methods how you can use better inference tools and and how you can enhance the credibility of your field at large not only your own research but but the whole field needs to move to higher level and i i think that no scientific field is perfect there are different stages of maturity at different stages of engagement with better methods and and this is happening in a continuous basis it's an evolution process so it's it's not at one time that we did one thing and then science is going to be clean and perfect from now on it is a continuous struggle and every day you can do things better or you can do things worse of those 35 million people who are out there publishing science today how many of them do you think are really fit to be principal investigators and be the ones that are making the decisions about where the resources go what the questions are that should be asked and what the real and final interpretation is i mean that has to be a relatively small fraction of that large number right well 35 million is the number of author ids in in scopus and even that one is a biased estimate like like any estimate it could be that you have a much much smaller number of people who are what we call principal investigators the the vast majority of people who have authored at least one scientific paper have just authored a single scientific paper and they have just been co-author so you know they may be students or staff or supporting staff in in larger enterprises and they never assume they're all of of leading research or designing research or being the the key players in doing research there's a much smaller core of people who i would call principal investigators we're talking probably at a global level if you take all sciences into account probably they're less than one million but still this is a huge number of course their level of training their level of how familiar they are with with best methods their their beliefs and and priors and biases it's very difficult to fathom some people argue that we need less research that probably we should cut back and really be more demanding in asking for credentials and for training and for methodological rigor for people to be able to lead research teams i'm a bit skeptical about any approach that is starting with a claim we need to cut back on research because i i think that research and science eventually is the best thing that that has happened to human sciences is the best thing that has happened to humans and i think that if we say we need to cut back on research because research is suboptimal we may end up in a situation where you create an even worse environment where you have even more limited resources and you still have all these millions of people struggling to to get these even more limited resources which means that they have even more incentives to cut corners they have even more incentives to come up with striking splashing results and then you have an even more unreliable literature so less is not necessarily the solution actually it may be problematic improved standards improved circumstances of doing research an improved environment of doing research is probably what we should struggle for creating the background where someone who's really a great scientist and knows what he or she is doing will get support and will be allowed to thrive also allow to look at things that have a high risk of failing i think that if we continue incentivizing people to get significant results no matter how that is defined we are incentivizing people to do the wrong thing we should incentivize them to to try really interesting ideas and to have a high chance of failing this is perfectly fine i think if you don't fail you're not going to succeed so we need to be very careful with interventions that happen at a science-wide level or even discipline-wide level we do not want to destroy science we want to improve science and some of the solutions they run the risk of doing harm sometimes based on your comment about the sort of the risk appetite that belongs in science to me it suggests an important role for philanthropy because industry obviously has a very clear risk appetite that is going to be driven by a financial return by definition everybody involved in that is a fiduciary whether it be to a private or public shareholder and therefore it's not the time to take risk for the sake of discovery conversely at the other end of that spectrum it might seem like the government in the pure public sector should be funding risk but given the legislative process by which that money is given out and the lack of scientific training that is in the people who are ultimately decision makers for that money it also seems like a sub-optimal place to generate risk that seems to be the place where you actually want to demonstrate a continued winning career even if you're not advancing knowledge in the most insightful way and so what that leaves is an enormous gap for risk which i think has to be filled with philanthropic work do you agree with that i agree that philanthropy is very important no strings attached philanthropy can really be catalytic in generating signs that would be very difficult to fund otherwise of course public funding is also essential and i think that we should make our best to make a convincing case that public funding should increase and you know not decrease as i said decreasing public funding makes things far far worse for many reasons i think that we need to realign some of our priorities on what is being funded with each one of these mechanisms currently a lot of public funding is given to generate translational products that are then exploited immediately by companies who make money out of them and conversely the testing of these products is paid by the industry i find that very problematic because the industry is financing and controlling the studies primarily randomized trials or other types of evaluation research that are judging whether these products that they're making money of are going to be promoted used become blockbusters and and so forth which inherently has a tremendous conflict i would argue that the industry should really pay more for the translational research for developing products through the early phases and then public funding should go to testing whether these products are really worth it whether they are beneficial whether they have benefits where they have no harms or very limited harms that research needs to be done with unconflicted funding and unconflicted investigators ideally through public funds of course philanthropy can also contribute to that philanthropy i think can play a major role in allowing people to pursue high-risk ideas and things that probably other funders would have a hard time to fund i think that public funds should also go to high risk ideas the public should be informed that science is a very high risk enterprise if if you try to create a narrative and i think that this is the the traditional narrative that money from taxpayers are used only for paying research grants that each one of them is delivering some important deliverables i think this is a false narrative most grants if if they really look at interesting questions they will deliver nothing or or at least you know they will deliver that sorry we tried we spent so much time we spent so much effort but we didn't really find something that is interesting we'll try again we did our best we had the best tools we had the best scientists we applied the best methods but we didn't find the new laws of physics we didn't find a new drug we didn't find a new diagnostic test we found nothing that should be a very valid conclusion if you do it the right way with the right tools with the right methods with the best scientists being involved putting down legitimate effort we should be able to say we found nothing but out of 1000 grants we have five that found something and and that's what makes the difference it's not that each one of them made a huge contribution it is these five out of 1000 in some fields and in other fields obviously maybe a higher yield that eventually transformed the world i mean this seems like a bit of a communications problem because that's clearly the venture capital model that seems to work very well which is on any given fund it your fund is made back by one company or one bet it's not an average it's a very asymmetric bet and similarly when you look at other landmark public high-risk funding things the manhattan project the space project these were upsettingly high-risk projects and yet i don't get the sense that the public wasn't standing behind those so it almost seems like there's a disconnect in the way scientists communicate their work to the public versus the way nasa did i mean nasa was a pr machine and obviously in the case of the manhattan project i think you know you're in the duress of war and but you know we can't lose sight of the fact that the scientific community was the one that stood up the physicists of the day are the ones that said to roosevelt like this has to be done i mean einstein took a stand so i don't know i guess it all comes back to scientists need to lead a bit and lead to be better communicators with the public right science communication is a very difficult business and i i think that especially in environments that are that are polarized that have lots of conflicts of inherent conflicts lots of stakeholders in in the community are trying to achieve the the most for themselves and for their own benefits it can be very tricky you know scientists have a voice but that voice is often drowned in the middle of all the screams and twitter and social media and media and and agendas and lobbies and and everything how do we strengthen that i think that there's two paths here one is to use the same tricks as lobbies do and the other is to stick to our guns and behave as scientists you know we're scientists we should behave as scientists i cannot prove that one is is better than the other i i think that both myself and many others feel very uneasy when we are told to really cross the borders of science and try to become communicators that are lobbying even for science it's not easy you want to avoid exaggeration you want to say that i don't know i'm doing research because i don't know i'm an expert but i don't know and this is why i believe that we need to know because these are questions that could make a difference for you how do you tell people that most likely i will fail that most likely 100 people like me will fail but maybe one will succeed we need to keep our honesty we need to make communication clear cut we need to also fight against people who are not scientists and who are promising much more and they would say that oh you need to do this because it will be clearly a success and they're not scientists but you know they're very good lobbies it's very difficult it's difficult times for science it's difficult times to defend science i think that we need to defend our method we need to defend our principles we need to defend the honesty of science in trying to communicate it rather than build exaggerated promises or narratives that are not realistic then even if we do get the funds we have just told people lies i completely agree i don't think what you and i are saying is mutually exclusive i think that's the point right i mean you said it a moment ago right i mean feynman's famous line that you know the most important rule in science is not to fool anyone and that starts with yourself you're the most you're the easiest person to fool and once you fooled yourself the game is over and i think the humility that you talk about communicating with the public is the necessary step i think people i mean i guess for me just having my daughter who's now just you know starting to understand you know or ask questions about science it's so much fun to be able to talk about this process of discovery and to remind ourselves that it's not innate right this is not an innate skill this is something this methodology didn't exist 500 years ago so for all but 0.001 of our genetic lineage we didn't even have this concept so that gives us a little bit of empathy for people who have no training because if you weren't trained in something you know it's you know there's no chance you're going to understand it without this explanation but i feel strongly that there can't be a vacuum right because the vacuum always gets filled and if the scientists aren't the ones speaking then you know if the good scientists aren't the ones speaking then it's either going to be the bad ones and or the charlatans who will before we leave eppie there's one thing i want to go back to that i think is another really interesting paper of yours this is one from two years ago this is the challenge of reforming nutritional epidemiologic research and this is the one where you looked at the single foods and the claims that emerged in terms of epidemiology i mean some of these things were simply absurd do you remember this paper that i'm talking about john you've written a couple along these lines but this is the one that that you know where you found a publication that suggested eating 12 hazelnuts per day extended life by 12 years which was the same as drinking three cups of coffee and eating one mandarin orange per day could extend lifespan by five years whereas consuming one egg would shorten it by six years and two strips of bacon would shorten life by a decade which by the way was more than smoking how do you explain these results and more importantly what does it tell us again about this process well these estimates obviously are tongue-in-cheek they're they're not real estimates they're a very crude translation of what the average person in the community would get if they see the numbers that are reported typically with relative risks in the communication of these findings they're not epidemiologically sound you know the true translation to change in life expectancy would be much smaller but even then they would probably be too big compared to what the real benefits might be or the real harms might be with these nutrients i think it just shows the the magnitude of the problem that if you have a system that is so complicated with so inaccurate measurements with so convoluted and overtly correlated variables with selective reporting and biases superimposed you get a situation pretty much like what we described in the nutrients and cancer risk where you get an implausible big picture where you're talking about huge effects that are unlikely to be true so it goes back to what we have been discussing about how you remedy that situation how you do you bring better methods and better training and better inferences to that land of irreproducible results now in gosh it might have been 2013-14 a very interesting study was published called predimed which we'll spend a minute on and it was interesting in that it was a clinical trial it had three arms and it relied on hard outcomes hard outcomes meaning mortality or morbidity of some sort rather than just soft outcomes like a biomarker if you had told me before the results came out this is the study you're going to have a low-fat arm and two mediterranean arms that are going to be split this way in this way and we're going to be looking at primary prevention i would have said the likelihood you'll see a difference in these three groups is quite low because it just didn't strike me as a very robust design but i guess to the author's credit they had selected people that were sick enough that within you know i think they had planned to go as long as seven or so years but under five years they ended up stopping the study given that the two arms in the mediterranean arm one that was randomized to receive olive oil the other i believe received nuts performed significantly better than the low-fat arm and that's sort of how the story went until a couple of years later what happened then so here you have a situation where i have to disclose my my own bias that i love the mediterranean diet and i have been a believer that this should be a great diet to use i mean i grew up in athens and obviously it's something that i enjoy personally a lot and i would be very happy to see huge benefits with it for many years i was touting these results as here you go you have a large trial that can show you big benefits on a clinical outcome and actually this is mediterranean diet which is the diet that i prefer personally even better and just to make the point it was it was both statistically and clinically very significant indeed a beautiful result very nice looking and i was very very happy with that i i would use it as an argument that here here's how you can do it the right way and and show clinically relevant uh results but but then it was realized that unfortunately this trial was not really a randomized trial the randomization had been subverted that a number of people had not actually been randomized because of problems in the way that they were recruited and therefore the data were problematic and you had a design where some of the trial was randomized and some of the trial was actually observational so in jungle journal medicine retracted and republished the study with lots of additional analysis that try to take care of that subversion of randomization in different ways excluding these people from the calculations and also using approaches to try to correct for the imposed observational nature of some of the data the results did not change much but it creates of course a very uneasy feeling that if really the creme de la creme trial the one that i adore then i admired had such a major problem you know such a major basic unbelievably simple problem in its very fundamental structure of how it was run how much trust can you put on other aspects of the trial that require even more sophistication and even more care for you know for example arbitration of outcomes or or how you count outcomes as you say this is a trial that originally was reported with limited follow-up compared to the original intention it was stopped at an interim analysis the trial has had lengthier follow-up it has published a very large number of papers as secondary analysis but still we lack what i would like to see as a credible result i mean it's it's a tenuous partly randomized trial and unfortunately doesn't have the same credibility now compared to what i thought when it was a truly randomized trial and there was one outcome that was reported and that seemed to be very nice now it's it's a partly randomized partly subverted trial with i don't know 200 300 publications floating around with uh with very different claims each time most of them looking very nice but but fragmented into that space of secondary analysis it doesn't mean that mediterranean diet does not work and i i still like to eat things that that fit to a mediterranean diet and this is my bias but it just gives one example of how things can go wrong even when you have good intentions i think that i can't see that people really wanted to do it wrong but one has to be very cautious yeah i mean i i think for me the takeaway if i remember some of the details which i might not i mean one of the big issues was the randomization around the inner household subjects right they wanted that you couldn't have people in the same house eating the different diets which is a a totally reasonable thought it just strikes me as sloppiness that it wasn't done correctly in the first place you know the the cost of doing a study the cost and duration of doing a study like that is so significant that it's just a shame that on the first go it's not you know it's not nailed because you know it could be seven years and a hundred million dollars to do that again this is true but uh one has to take into account that in such an experiment you have a very large number of people who are involved and their level of methodological training and their ability to understand what needs to be done may vary quite a bit so it's very difficult to secure that that everyone involved in all the sites involved in the trial would do the right thing and i think that this is an issue also for other randomized trials that are multicenter very often now we realize that because of the funding structure since as we said there's very little funding from public agencies most of the multi-center trials are done by the industry they try to impose some rigor and some standards but they also have to recruit patients from a very large number of sites sometimes from countries and from teams that have no expertise in clinical research and then you can have situations where a lot of the data may not necessarily be fraudulent but they're collected by people who are not trained who have no expertise who don't know what they're doing and sometimes depending on the study design especially with unmasked trials or trials that lack allocation concealment or both you can have severe bias interfere even in studies that seemingly appear to be like the creme de la creme of large-scale experimental research yeah john let's let's move on to one last topic at least for now which is the events of 2020. in early april i had this idea talking with someone on my team which was boy the zero prevalence of this thing might be far higher than the confirmed cases of this thing and if that were true it would mean that the mortality from this virus is significantly lower than what we believe this was at a time when i think there was still a widespread belief that five to ten percent of people infected with this virus would be killed and there were basically a non-stop barrage of models suggesting two to three million americans would die of this by the end of the year the first person i reached out to was david allison and i said hey david what do you think about doing an assessment of zero positivity in new york city and he said let's call john ionides so we gave you a call that afternoon it was a saturday afternoon we all hopped on a zoom and you said well guess what i i'm doing this right now in santa clara and i don't think it had been published yet right i mean i think you had just basically got the data right i believe that it was about that time yes tell me a little bit about that study and what what did it show because it was certainly one of the first studies to suggest that basically the zero positivity was much higher than the confirmed cases this is a pair of two studies actually one was done in santa clara and the the other was done in la county and both of them the design aim to collect a substantial number of participants and try to see how many of them had antibodies to the virus which means that they had been infected perhaps at least a couple weeks ago and there were studies that aaron ben david then jay batacharia led and also we had colleagues from the university of southern california also leading the the study in in la county they were studies that i thought were very important to do i was just one of many co-investigators but i feel very proud to have worked with that team they were very devoted and they really put together in the field an amazing amount of effort and very readily could get some results that would be very useful to tell us more about how widely spread the viruses the results i'm not sure whether you would call them surprising shocking anticipated it depends on what your prior would be personally i was open to the possibilities of any result i i had no clue how widely spread the virus would be and this is why i thought these studies were so essential i had already published more than a month ago that by that time that we just don't know we just don't know whether we're talking about a disease that is very widely spread or very limited in its spread which also translates in an inverse mode to its infection fatality rate if it's very widely spread the infection fatality rate per person is much lower if it is uh very limited in its spread it means that fewer people are affected but the infection fatality rate would be very high so whatever the answer would be it would be an interesting answer and i the the result was that the virus was very widely spread far more common compared to what we thought based on the number of tests that we were doing and the number of pcr documented cases at that time in the early months of the pandemic we were doing actually very few tests so it's not surprising at all that the under ascertainment would be huge i think that once we started doing more tests and or in countries that did more testing the under ascertainment was different compared to places that were not doing much testing or were doing close to no testing at all i think that the result was amazing i i felt that that was a very unique moment seeing these results when i i first saw that that's what we got that it was about 50 times more common than we thought based on the documented cases but obviously generated a lot of attention and a lot of animosity because people had very strong priors i think it was very unfortunate that all that happened in a situation of a highly polarized toxic political environment somehow people were aligned with different political beliefs as if you know a political belief should also be aligned with with a scientific fact it's it was it was just completely horrible so it created massive social media and media attention both good and bad and i think that we were bombarded with comments both good and bad and criticism i'm really grateful for the criticism because obviously these were very delicate results that we had to be sure that we had the strongest documentation for what we were saying and we went through a number of iterations to try to address these criticism in the best possible way in the long term with several months down the road inside we we see that these results are practically completely validated we have now a very large number of prevalent studies that have been done in very different places around the world we see that those studies that were done in early days had as i said the the worst under ascertainment we had tremendous under ascertainment in several places around the world even in santa clara there's another data set that was included in the national survey of a study that was published in the lancet about a month ago on hemodialysis patients and the infection rate if you translated that was a couple of months after our study if you translate it to an infection fatality rate is exactly identical to what we had observed in early april so the study has been validated it has proven that the viruses is a very rapidly and very widely spreading virus and you need to deal with it based on that profile it is a virus that can infect huge numbers of people my estimate is as of early december probably we may have close to 1 billion people who have already been infected you know more or less around the world and there's a very steep risk gradient there's lots of people who have practically no risk or minimal risk of having a bad outcome and there are some people who have tremendous risk of being devastated we have for example people in nursing homes who have 25 infection fatality rate you know one out of four of these people if they're infected they will die so it was one of the most interesting experiences in my career both of the fascination about seeing these results and also the fascination [Laughter] and and some of the intimidation of some of the the reaction to these results in a in a very toxic environment unfortunately i don't necessarily mean by name but what forces were the most critical presumably these would be entities or individuals that wanted to continue to promote the idea that that the risk here warranted greater shutdown slow down help me understand a little bit more where some of the vitriol came from i think that there were many scientists who made useful comments and and as i said i'm very grateful for these comments because they helped improve the paper and then there were many people in social media that include some scientists who actually however were not epidemiologists unfortunately in the middle of this pandemic we have seen lots of scientists who have no relationship to epidemiology become kind of twitter or facebook epidemiologists all of a sudden and you know have very vocal opinions about how things should be done i i remember a scientist who was probably working in physics or not who was sending emails every two hours to the principal investigators and and i i was cc'd in them saying you have not corrected the paper yet uh and every two hours you know you have not corrected the paper yet i mean his comment was wrong to start with but as we were working on revisions as you realize uh we did that with ultra speed responding within record time to create a revised version and to post it but even posting it takes five days more or less but what do you think was at the root of this anger directed towards you and the team unfortunately i think that the the main reasons were not scientific i think that most of the animosity was related to the toxic political environment at the moment and personally i feel that it is extremely important to to completely dissociate science from politics science should be free to say what has been found with all the limitations and all the caveats but you know be precise and accurate i would never want to think about what a politician is saying in a given time or given circumstances and then modify my findings based on what one politician or another politician is saying so i think that one of the attacks that i received was that i i have conservative ideology which is like the the most stupendous claim that i can think of you know looking at my track record and how much i have written about climate change and climate urgency and emergency and the problem with with gun sales and actually you know gun sales becoming worse in the environment of the pandemic and the need to promote science and the need to diminish in injustice and the need to provide health good health to all people and to decrease poverty you know claiming me that i'm a supporter of conservative ideology sick conservative ideology is completely weird and then smearing of all sorts that the owner of an airline company had given five thousand dollars to stanford which i was not even aware of the the funding of the trial which i was not even the pi was through a crowdsourcing mechanism going to the stanford development office which i never heard of who were the people who had funded that and of course none of that money came to me or to all the other investigators who completely volunteered our time we have received zero dollars for for our research but tons of smearing sorry just to clarify john you're saying the accusation was that because an airline had contributed five thousand dollars to stanford for which you saw none of it that your assessment was really a way to tell everybody that the airlines should be back to flying yes but you know i i heard about it when when the the bus speed reporter yeah yeah of course yeah yeah of course so it's it's very weird and you know because of all the attacks that that we received you know i received tons of emails that were hate mail and some of them threatening to me and my family my mother she's 86 years old and there was a hoax circulated in social media that she had died of coronavirus and her friends started calling at home to ask when the funeral would be and when she heard that from multiple friends she had a life-threatening hypertensive crisis so these people really had a very toxic response that did a lot of damage to me and to to my family and and to others as well and i think that it was very unfortunate i asked stanford to try to find out what was going on and there was a fact-finding process to try to realize you know what why is that happening and of course it concluded that there was absolutely no conflict of interest and and nothing that had gone wrong in terms of any potential conflict of interest but but this doesn't really solve the more major problem for me the most major problem is how do we protect scientists it's not about me it is about other scientists some of them even more prominently attacked i think one example is tony fauci he was my supervisor i have tremendous respect for him he was my supervisor when i was at nyad at nih he's a brilliant scientist he has been ferociously attacked there's other scientists who are much younger they're not let's say as powerful they will be very afraid to disseminate their scientific findings objectively if they have to ponder what the environment is at the moment and what do different politicians say and how will my results be seen we need to protect those we need to protect people who would be very much afraid to talk and they would be silenced if we see examples that uh you know can you see what happened to johnny and eddie's or what happened to tony fauci if i were to to say something i would be completely devastated so i think that we need to be tolerant we need to give science an opportunity to do its job to find useful information to correct mistakes or improve on methods i mean this is part of the scientific process but not really throw all that smearing and all that vicious vitriol to scientists it's it's very dangerous regardless of whether it comes from people in one or another political party or or one in another ideology it ends up being the same it ends up being populist attacks of the worst possible sort regardless of whether they come from the left or right or middle or or whatever part of the political spectrum well i'm very sorry to hear that you had to go through that especially at the level of your family i was actually i knew that you had been attacked a little bit i i was not aware that it had it had spread to the extent that you described it what do we do going forward here i mean it still seems to be a largely partisan issue there's a very clear left versus right approach to this that seems mostly science agnostic i i think it's viewed as unwise to have a changing opinion outside of science right i mean in science that's a hallmark of a great thinker right someone who can change their mind in the presence of new information that's that's a that's a core competency of doing good science in fact much of what we've spoken about today is the toxicity of not being able to update your priors and change your mind in the face of new information but yet somehow in politics that is considered the biggest liability of all time somehow in politics anytime you change your mind it's wishy-washy and you're weak and you don't know your ideology there seems to be an incompatibility here and in a crisis moment like this which is a this was a crisis that seems to bring these things to the fore right it is true and i don't want to see that in a negative light necessarily because somehow the coronavirus crisis has brought science to the limelight in some positive ways a way you know if i think that people do discuss more about science it has become a topic of great interest people see that their lives depend on science they feel that their world depends on science what will happen in the immediate future and and mid-range future depends on science and how we interpret science and how we use science so in a way suddenly we have had hundreds of millions if not billions of people become interested in science acutely but obviously most of those unfortunately given our horrible science education they have no science education and they use the tools of their traditional society discourse which is largely political and sectorized to try to deal with scientific questions and and this is an explosive mix i think it creates a great opportunity to communicate more science and better science at the same time it makes science a a hostage of all these lobbying forces and all of this turmoil that is happening in the community well john what are you most optimistic about i mean you're you have you have lots of time left in your career you're gonna go on and do many more great things you're gonna you're gonna you're gonna be a provocateur what are you most excited and optimistic about in terms of the future of science and the type of work that you're that you're looking to advance well i'm i'm very excited to make sure that and it does happen that there's so many things that i don't know and every day i realize that there's even more things that i don't know i think that so far if if that continues happening and every day i can find out about more things that i don't know things that i thought were so but actually they were wrong and i need to correct them and find ways to correct them then i really look forward to to a good future for science and a good future for humans i think that we're just at the beginning we're just at the beginning of of knowledge and i feel like a little kid who just wants to learn a little bit more a little bit more each time well john the last time we were together in person we were in palo alto when we had a mediterranean dinner so i hope that i hope that sometime in 2021 that'll bring us another chance for another flaky white fish and some lemon potatoes and whatever other yummy things we had that evening that would be wonderful and i hope that it does increase life expectancy as well although even if it doesn't i think it's worth it john thanks so much for your time today thank you peter thank you for listening to this week's episode of the drive if you're interested in diving deeper into any topics we discuss we've created a membership program that allows us to bring you more in-depth exclusive content without relying on paid ads it's our goal to ensure members get back much more than the price of the subscription to that end membership benefits include a bunch of things one totally kick-ass comprehensive podcast show notes that detail every topic paper person thing we discuss on each episode the word on the street is nobody's show notes rival these monthly ama episodes or ask me anything episodes hearing these episodes completely access to our private podcast feed that allows you to hear everything without having to listen to spiels like this the qualis which are a super short podcast that we release every tuesday through friday highlighting the best questions topics and tactics discussed on previous episodes of the drive this is a great way to catch up on previous episodes without having to go back and necessarily listen to everyone steep discounts on products that i believe in but for which i'm not getting paid to endorse and a whole bunch of other benefits that we continue to trickle in as time goes on if you want to learn more and access these member-only benefits you can head over to petertiamd.com forward slash subscribe you can find me on twitter instagram and facebook all with the id peteratiammd you can also leave us a review on apple podcast or whatever podcast player you listen on this podcast is for general informational purposes only does not constitute the practice of medicine nursing or other professional health care services including the giving of medical advice no doctor-patient relationship is formed the use of this information and the materials linked to this podcast is at the user's own risk the content on this podcast is not intended to be a substitute for professional medical advice diagnosis or treatment users should not disregard or delay in obtaining medical advice from any medical condition they have and they should seek the assistance of their healthcare professionals for any such conditions finally i take conflicts of interest very seriously for all of my disclosures and the companies i invest in or advise please visit petertiammd.com forward slash about where i keep an up-to-date and active list of such companies [Music]

[01:52:11] you