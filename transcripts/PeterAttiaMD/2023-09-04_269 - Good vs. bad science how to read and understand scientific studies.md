# 269 - Good vs. bad science: how to read and understand scientific studies

**Channel:** Peter Attia MD
**Upload Date:** 2023-09-04
**URL:** https://www.youtube.com/watch?v=Vnzodlb8rzE
**Duration:** 122 minutes

## Description

Watch the full episode and view show notes here: https://bit.ly/47VnNbz
Become a member to receive exclusive content: https://peterattiamd.com/subscribe/
Sign up to receive Peter's email newsletter: https://peterattiamd.com/newsletter/

This special episode is a rebroadcast of AMA #30, now made available to everyone, in which Peter and Bob Kaplan dive deep into all things related to studying studies to help one sift through the noise to find the signal. They define various types of studies, how a study progresses from idea to execution, and how to identify study strengths and limitations. They explain how clinical trials work, as well as biases and common pitfalls to watch out for. They dig into key factors that contribute to the rigor (or lack thereof) of an experiment, and they discuss how to measure effect size, differentiate relative risk from absolute risk, and what it really means when a study is statistically significant. Finally, Peter lays out his personal process when reading through scientific papers.

We discuss:
0:00:00 - Intro
0:00:18 - The ever-changing landscape of scientific literature
0:03:19 - The process for a study to progress from idea to design to execution
0:06:39 - Various types of studies and how they differ
0:19:14 - The different phases of clinical trials
0:28:32 - Observational studies and the potential for bias
0:49:27 - Experimental studies: randomization, blinding, and other factors that make or break a study
1:03:49 - Power, p-values, and statistical significance
1:16:21 - Measuring effect size: relative risk vs. absolute risk, hazard ratios, and “number needed to treat”
1:26:39 - How to interpret confidence intervals
1:34:15 - Why a study might be stopped before its completion
1:43:01 - Why only a fraction of studies are ever published and how to combat publication bias
1:53:45 - Why certain journals are more respected than others
1:57:54 - Peter’s process when reading a scientific paper

--------
About:

The Peter Attia Drive is a deep-dive podcast focusing on maximizing longevity, and all that goes into that from physical to cognitive to emotional health. With over 70 million episodes downloaded, it features topics including exercise, nutritional biochemistry, cardiovascular disease, Alzheimer’s disease, cancer, mental health, and much more.

Peter Attia is the founder of Early Medical, a medical practice that applies the principles of Medicine 3.0 to patients with the goal of lengthening their lifespan and simultaneously improving their healthspan. 

Learn more: https://peterattiamd.com

Connect with Peter on:
Facebook: http://bit.ly/PeterAttiaMDFB
Twitter: http://bit.ly/PeterAttiaMDTW
Instagram: http://bit.ly/PeterAttiaMDIG

Subscribe to The Drive:
Apple Podcast: http://bit.ly/TheDriveApplePodcasts
Overcast: http://bit.ly/TheDriveOvercast
Spotify: http://bit.ly/TheDriveSpotify
Google Podcasts: http://bit.ly/TheDriveGoogle

Disclaimer: This podcast is for general informational purposes only and does not constitute the practice of medicine, nursing, or other professional healthcare services, including the giving of medical advice. No doctor-patient relationship is formed. The use of this information and the materials linked to this podcast is at the user's own risk. The content on this podcast is not intended to be a substitute for professional medical advice, diagnosis, or treatment. Users should not disregard or delay in obtaining medical advice for any medical condition they have, and they should seek the assistance of their healthcare professionals for any such conditions. I take conflicts of interest very seriously. For all of my disclosures and the companies I invest in or advise, please visit my website where I keep an up-to-date and active list of such companies. For a full list of our registered and unregistered trademarks, trade names, and service marks, please review our Terms of Use: https://peterattiamd.com/terms-of-use/

## AI Summary

Here's my comprehensive analysis of this episode:

1. **Executive Summary**:
This episode focuses on how to critically evaluate scientific studies and research papers. Dr. Attia and Bob discuss the fundamentals of scientific research, including study design, statistical analysis, and publication processes. They emphasize the importance of understanding both the technical aspects (like p-values and confidence intervals) and potential biases in research. The discussion is particularly relevant for anyone trying to make sense of medical literature and scientific news.

2. **Key Medical/Scientific Points**:
- Three main categories of studies: observational, experimental, and meta-analyses [00:07:38]
- Clinical trial phases (1-4) and their purposes [00:21:55]
- Power analysis importance in study design [00:08:40]
- Hazard ratios and their interpretation [00:19:52]
- Publication bias and its impact on scientific literature [01:47:01]

3. **Health Optimization Tips**:
Universal recommendations:
- Be skeptical of media reporting on scientific studies [01:33:52]
- Look for both absolute and relative risk in study results [01:17:16]

Context-specific recommendations:
- Consider the number needed to treat (NNT) when evaluating treatments [01:21:55]
- Evaluate study population characteristics for relevance [01:00:16]

4. **Supplements & Medications**:
No specific supplements were discussed in detail, but the episode covered how to evaluate drug trials and their results.

5. **Exercise & Movement**:
No specific exercise protocols were discussed.

6. **Nutrition & Diet**:
- Discussion of food frequency questionnaires and their limitations [00:33:00]
- Example of PREDIMED study and Mediterranean diet research [00:50:54]

7. **Biomarkers & Testing**:
- Importance of pre-specified primary and secondary outcomes [01:45:38]
- Discussion of surrogate endpoints versus clinical endpoints [01:17:16]

8. **References & Resources**:
- Clinicaltrials.gov for study pre-registration [01:49:43]
- Center for Open Science and registered reports [01:50:59]
- Major medical journals and their impact factors [01:54:40]

9. **Notable Quotes**:
"A thousand sows ears makes not a pearl necklace" - regarding meta-analyses of poor quality studies [01:19:56]

10. **Follow-up Questions**:
- How can the peer review process be improved?
- What role should pre-print servers play in scientific publishing?
- How can we better address publication bias in medical research?

The episode provides a comprehensive framework for evaluating scientific literature, particularly useful for healthcare professionals and interested laypeople trying to navigate medical research.

## Transcript

[00:00:01] hey everyone welcome to the drive podcast I'm your host Peter attia hey Bob how are you man looking pretty studious there in the library today hey Peter thanks very much yeah just getting some reading in before the before the podcast well this is uh this is gonna be a pretty good one because um as you may recall about um I don't know four or five months ago maybe longer I was on a podcast with Tim

[00:00:29] Ferriss and uh I don't know how it came out but I do remember somehow it came up that um you know we had spent a lot of time writing this series studying studies and uh God that's been four years ago I think um yeah but we didn't really have something more digestible for folks on how to make sense of the ever-changing landscape of scientific literature and uh you know in some ways how to how to kind of distinguish between the signal and the noise of of the research news cycle um and I remember after that Tim and I went out for dinner and he kept sort of pressing me on well what can I do to get better at this process um you know are there newsletters I said

[00:01:13] I should be subscribing to and things like that and while I'm sure that there are I didn't know what they were at the top of my head um and so I think what we've done here when I say we I mean you um what what you have done here is kind of aggregate all the questions that have come in over the past year basically um that pertain to understanding the structure of Science and uh I really I don't know I look through the questions uh last week and I was pretty excited I think it's gonna be a sweet discussion and I hope this serves as an amazing primer for people to to really understand the process of scientific experiments and and everything from how how studies were published and and obviously what some of the limitations are so anything else you want to add to that Bob before we jump in

[00:02:02] I agree I think it's a fun topic we get so many we get so many of these questions that we end up earliest I do um or to the website where we'll Point readers to one of the parts of the studying studies but I think sometimes just talking about it and explaining it can help a lot so I think this will be really useful um as far as like a question and answer session rather than just a just reading a Blog yeah and and again I don't think this displaces that other stuff I think we go into probably more detail on some things there but I also think we're going to cover things here that aren't covered there so um depending on how you like to get your info this uh this could be fun so where where do you want to start so I think that we have again a lot of questions but I think the this question gets to the core of of I think what we're trying to do here which is how can a user or a person who has no scientific background better understand studies that they read in the news or in the Publications to know if the findings are solid solid or not especially in today's age where you can easily see two studies the contradictory contradict each other coffee's good coffee's bad eggs are good eggs are bad um so I thought we could run through uh a bunch of questions with the first one um that we got here is what is the process for a study to go from an idea to design an execution yeah it is a great this is a great question um

[00:03:33] I mean in theory it should start with a hypothesis uh good science is generally hypothesis driven um so I think the cleanest way to to think about that is to take the position that there is um no relationship between um two phenomena so we would call this sort of a null hypothesis so um my hypothesis might be that drinking coffee makes your eyes turn darker so I would have to state that hypothesis and then I would have to frame it in a way that says my null hypothesis is that when you drink coffee your eyes do not change in color in any way shape or form and that would imply that the alternative hypothesis is that when you drink coffee your eyes do change color you can already see by the way that there's Nuance to this because am I specifying what color it changes to does it get darker does it get lighter does it change to blue green does it just get the darker shade of whatever it is but let's put that aside for a moment and just say that you will have this null hypothesis and you'll have this alternative hypothesis and to be able to formulate that cleanly is sort of the first step here the second thing of course is to conduct an experimental design how are you going to test that hypothesis um as we're going to talk about a really really elegant way to test this is using a randomized controlled experiment um if it's possible to Blind it we'll talk about what that means but you know and you'll have to decide well how long should we make people drink coffee how frequently should they drink coffee how are we going to measure eye color these are the questions that come down to experimental design you then have to determine a very important variable which is how many subjects will you have and of course I will depend on a number of things including how many arms you will have in this study but it comes down to doing something that's called a power analysis and this is so important that we're going to spend some time talking about it today although I won't talk about it right now if this study involves human subjects or animal subjects you will have to get something called an

[00:06:02] Institutional review board to approve the ethics of the study so you'll have to get that IRB approval um you'll have to determine what your primary and secondary outcomes are get the protocol approved develop a plan for statistics and then pre-register the study all of these things happen before you do the study and of course in parallel to this you have to have funding so those are kind of the steps that go into doing an experimental study and what we're going to talk about I think in a minute is that there are some studies that are not experimental uh where some of these steps are obviously skipped yeah one of the one of the questions we got was what are the different types of studies out there and what do they mean for example you know an observational study versus a randomized controlled study what are the different types of studies yeah so I mean I think broadly speaking you can break studies into three categories um one would be observational studies and we'll we'll bifurcate those or try for Kate those in a minute then you can have experimental studies and then you can have basically summations of and or reviews of and or analyzes of studies of any type so um let's kind of start at the bottom of that pyramid um and I think you actually have a figure that I don't like very much but I know what to say yeah that was one of your favorites yeah I can't stand it

[00:07:38] I'll tell you what I like about the figure I like the color schema because my boys are so obsessed with rainbows that if I show them this figure they're going to they're going to be really happy so let's let's pull up said rainbow figure okay got it okay so uh you can sort of see these these these buckets here and and again at the at the at the level of kind of talking about them I think this makes sense what I don't agree with the pyramid for Bob is that it puts a hierarchy in place that suggests a meta-analysis is better than a randomized control trial uh which is not necessarily true but let's just kind of go through what each of these things mean so so looking at the observational studies an individual case report is uh you know look I I think the set first or second paper I ever wrote in my life when I was in medical school was an individual case report it was a patient who had come into um clinic this was it when I was at the

[00:08:40] NIH and this was a patient with metastatic melanoma and their calcium was Sky High dangerously high in fact and obviously our first assumption was that this patient had a metastatic disease to their bone and that they were lysine bone and calcium was was you know bleaching into their bloodstream turned out that wasn't the case at all um it turned out they had something that had not been previously reported in patients with melanoma which was they had developed this Pete parathyroid hormone related like hormone in response to their melanoma so this is a hormone that exists normally but it doesn't exist in this format and so their cancer was causing them to have more of this hormone that was causing them to raise their calcium level it was interesting because it had never been reported before in the literature and so I wrote this up this was an individual case report is there any value in that sure there's some value in that the next time a patient with melanoma shows up to clinic and their calcium is Sky High and someone goes to the literature to search for it they'll see that report and uh you know it will hopefully save them time in getting to the diagnosis so your mentor and friend Steve Rosenberg I think of him when I think of individual case reports where

[00:10:04] I think if you listen to the podcast he talks about this but a lot of what motivated him early on I think we're we're just a couple of a couple of cases and it I think it gets back to that first question too about the process for a study to go to an idea to design to execution which is to to have a hypothesis you need to make an observation so you make an observation you say hmm that's strange and I think that that that's what individual case reports can represent sometimes is this is a really this is an interesting observation it's hypothesis generating for the most part but it really it might kick-start a larger trial or it might might kick-start a career you never know yeah exactly now of course it's not going to be generalizable right it's I can't make any statement about um you know the frequency of this in the broader subset of patients and obviously I can't make any comment about any intervention that may or may not change the outcome of this so that gets us to kind of our next thing which is like a a case series uh or or set of studies so here you're basically doing the same sort of thing um but but in in in plural effectively um so so you you wouldn't just look at one patient you would say well um you know I've now been looking back at my clinical practice and I've had you know 27 patients over the last 40 years that have demonstrated this very unusual finding and another example of this going back to the Steve Rosenberg case would be uh one could write a paper that looks at all spontaneous regressions of cancer well there's obviously been a number of them they're infrequent um in fact they're exceedingly rare but there are certainly enough of them that one could write a case series so so now let's consider cohort studies so so cohort studies are larger studies and they can be retrospective or they can be prospective so I'll give you an example of both so a a retrospective observational cohort study would be like let's go back and look at all the people who have used saunas for the last 10 years and look at how they're doing today relative to people who didn't use saunas over the last 10 years so it's retrospective we're looking backwards it's observational we're not we're not doing anything right we're not telling these people to do this or telling those people to do that um and the hope when you do this is that you're going to see some sort of pattern undoubtedly you will see a pattern of course the question is will you be able to establish causality in that pattern cohort studies can just as easily although more time consumingly be prospective so you could say I want to follow people over the next five years ten years who use saunas and compare them to a similar number of people who don't and now in a forward-looking fashion we're going to be examining the other behaviors of these people and ultimately what their outcomes are do they have different rates of death heart disease cancer Alzheimer's disease other metrics of Health that we might be interested in again we're not intervening there's not an experiment per se we're just observing but now we're doing it as we

[00:13:36] March forward through time okay so this brings us to the kind of the next layer of this pyramid which are the experimental studies uh and again we sort of divide these into randomized versus non-randomized and of course this idea of randomization is going to be a very important one as we go through this so a non-randomized trial uh sometimes gets referred to as an open label trial where you take two groups of people and you give one of them a treatment and you give the other one either a placebo or a different treatment but you don't randomize them there's a reason that they're in that group so you might say um you know we want to study the effect of a certain antibiotic on a person that comes in the ER and we're going to take all the people that come in who look a certain way maybe they have a certain a fever of a certain level or a white blood cell count of a certain level and we're going to give them the antibiotic and the people who come in but they don't have those exact signs or symptoms we're going to not give an antibiotic to and we're going to follow them that's kind of a lame example you could do the same sort of thing with surgical interventions we're going to try to ask the question is surgery better than antibiotics for appendicitis or suspected appendicitis but we don't randomize the people to the choice there's some other factor that is going to determine whether or not we do that and as you can see that's going to have a lot of limitations because presumably there's a reason you're making that decision and that reason will undoubtedly introduce bias so of course the gold standard that we always talk about is a randomized control trial where whatever question you want to study you study it but you attempt to take all bias out of it by randomly assigning people into the treatment groups the two or more treatment groups um we'll talk about things like blinding later because you can obviously get into more and more rigor when you do this but before we leave the kind of experimental side or anything you want to add to that

[00:15:54] Bob uh I would add some non-randomized controlled trials maybe another example I'm trying to think of another example which is you have patience maybe maybe making a decision beforehand which will get into selection bias but if you people might want to go they might want to go on a Statin let's say and then other you give them a choice and the other ones might want to go on some other drug like a zetamine and they're they're they're basically selecting themselves into two groups but you could you could compare those two groups and see how they do um but you haven't it hasn't been randomized there's a there's a lot of bias that can go into that there could be a lot of reasons why one group is selecting a particular treatment over the other and so that's what I think when we get to randomized trials that it shows the power of randomization yeah yeah exactly um but let's we don't need to go back to the figure but people might recall that at the top of that pyramid was you know systemic reviews and meta-analyzes let's just talk about meta-analyzes since they um are probably the most powerful so this is a statistical technique where you can combine data from multiple studies that are attempting to look at the same question basically so um each study gets a relative waiting um and the weighting of a study is sort of a function of its Precision it depends a little bit on sample size other events in the study larger studies which have smaller standard errors are given more weight than smaller studies with larger standard errors for example um a very common way that these you'll you'll know you're looking at a meta-analysis we should have had a figure for this but I'll describe it the best I can they usually have a figure somewhere in there that will show kind of across rows all of the studies so let's say there's 10 studies included in the meta-analysis and then they'll have the hazard ratios for each of the studies so they'll represent them usually as little triangles so the the triangle will represent the 95 confidence interval of what the hazard ratio is which we'll talk about a hazard ratio but it's it's a basically a marker of the risk and you'll see all 10 studies and then they'll show you the final summation of them at the bottom which of course you wouldn't be able to deduce looking at the figure but it takes into account that mathematical weighting so on the surface meta-analyzes seem really really great right because if one trial one randomized trial is good ten must be better um but again I I know I've said this before probably three or four times over the past few years on the podcast um but as as James Yang one of the smartest people I ever met when when I was uh both a student and fellow at NCI uh once said during a journal Club about a meta-analysis that was being presented he said something to the effect of a thousand sows ears makes not a pearl necklace and you know that that's just an eloquent way to say that garbage in garbage out right so if you do a meta-analysis of a bunch of garbage studies you get a garbage meta-analysis so it it can't uh it can't clean garbage it simply can aggregate it so a meta-analysis of great randomized control trials will produce a great meta-analysis so I'm sure they try to control for garbage you know but the researchers and the investigators but I think to your point with the pearl necklace imagine if you had say 10 10 trials and nine of them are garbage one of them is really good really rigorous randomized controlled trial and you're looking at the top of the pyramid and you're saying well meta-analysis is the best we should be looking at this meta-analysis meanwhile you've got that one randomized controlled trial that actually is worth its salt it's it's rigorous Etc that that I would say if you had the option I think you probably would rely more on that one randomized control trial which is lower in the pyramid so I think that's probably I think you've told me one of your

[00:19:52] Hang-Ups with uh with the pyramid because it's not necessarily top of the pyramid is going to be some meta-analysis of of randomized control trials that's right yeah and and so I don't want to suggest meta-analyzes are not great what I want to suggest is you can't just take a meta-analysis as gospel without actually looking at each study you don't get a pass at examining each of the constitutive studies within a meta-analysis is is really the point I think we want to make here yeah there's there's one thing in here that that isn't represented but we had a few questions about it I think a couple people are asking about you know you know what's the difference between a phase three and a phase two or a phase one clinical trial do you know what's going on there yes uh and these are great questions so so here we're talking about human clinical trials and and this this phraseology is is is used by the FDA um here in the United States and and typically um the the world does tend to follow in lockstep but not always with with kind of the fda's process so um if you go way way way back uh you you have an interesting idea you you have a drug that you think is or a molecule that you think will have some you know benefit well let's just think of it as a cancer therapeutic um and you've uh you've done some interesting experiments in animals and you know you maybe started with some mice and you went up to some rats and maybe even you've done something in primates and now you're really sort of committed to this as uh you the successiveness and the safety of this in animals looks good so it's both safe and efficacious in animals and you now decide you want to foray into the human space well the first thing you have to do is um file for something called an IND an investigational new drug application so after you do all of this pre-clinical work you have to file this IND with the

[00:21:55] FDA and that basically sets your intention of testing this as a drug in humans and the first phase of that which is called phase one is geared specifically to dose escalate this drug from a very very low level so to determine what the toxicity is across a range of doses that will hopefully have efficacy so these are typically very small studies uh usually less than 100 people and they're typically done in cohorts so you might say well the first you know 12 people are going to be at 0.1 milligrams per kilogram and assuming we see no adverse effects there we'll go up to 0.15 milligrams per kilogram for the next 12 people and if we have no issues there we'll escalate it to 0.25 now you'll notice Bob I said nothing in there about does the drug work right so right these are going to be patients with cancer if this is a drug that's being sought as a treatment for colon cancer these are going to be patients that all have colon cancer they're often going to be patients who have metastatic colon cancer right so these are going to be patients who have progressed through all other standard treatments uh and who are basically saying look sign me up for this clinical trial I realize that this first phase is not going to be necessarily giving me a high enough dose that I could experience a benefit and that you're really only looking to make sure that this drug doesn't hurt me but nevertheless I want to participate in this trial so if the if the drug gets through phase one safely then it goes to phase two and the goal of phase two is to continue to evaluate for safety but also to start to look for efficacy but this is done in an open label fashion what that means is they're not randomizing patients to One

[00:23:59] Drug versus the other typically they can but usually it's now we think we know one or two doses that are going to produce efficacy they were deemed safe in the phase one we're now going to take patience and give them this drug and look for an effect and a lot of times if there's no control arm in the study you're going to compare to the Natural

[00:24:24] History so let's assume that we know that patients with metastatic colon cancer have on standard of care have uh you know a median survival of X months well we're going to give these patients this drug and see if that extends it anymore and of course you could do this with a control arm but now it adds the number number of patients to the study so again typically very small studies can be you know in the 20 30 40 50 range maybe up to a few hundred people and that one Peter I think is a probably a good example of if you have the non-randomization this might be a case where say it's an immunotherapy and and people know about the immunotherapy and it's been really effective for a particular it's approved for a particular cancer let's say and there are a lot of people that that know about it and there are cancer patients that know about it and they they want to get that treatment but it's not approved if they're talking to their doctor or they they or maybe they're online they might enroll in one of these trials because they really want to try the drug and maybe they you know they might believe in it more than than some other treatment yep yeah there are lots of things that can introduce bias to a phase two if it does not have randomization again the goal would be to still randomize in phase two um because you really do want to tease out efficacy so if a study uh if a compound succeeds in phase two which means it continues to show no significant adverse safety effects which by the way doesn't mean it doesn't have side effects right every treatment has side effects it's just that it doesn't have um side effects that are deemed unacceptable for the risk profile of the patient and it shows efficacy so really you have to have these two things you then proceed to phase three and here a phase three is a really rigorous trial this is a huge step up it's typically a log step up in the number of patients so you'd you know you're talking potentially thousands of patients here um and this is absolute absolutely a placebo controlled trial or not necessarily Placebo but it can be sort of standard of care versus standard of

[00:26:34] Care Plus this new agent but it is randomized whenever possible it is blinded and with drugs that's always possible and these are typically longer studies so so now because you have so much more sample size you're going to potentially pick up side effects that weren't there in the first place and of course now you really have that gold standard for measuring efficacy and it's on the basis of the phase one phase two and mostly phase three data that a drug will get approved or not approved for broad use which leads to a fourth phase which is a post-marketing study so phase four studies take place after the drug has been approved and they're used to basically get additional information because once a drug is approved you now have more people taking it and they may also be using this to look at other indications for the drug so we talked about this recently right a phase four trial with semi-glutide being used to look at obesity versus its original phase 3 trials which we're looking at diabetes so kind of that's a the Drug's already been approved the study isn't being done to ask the question so should semi-glutide be on the market no it's on the market it's basically expanding the indication for semi-glutide in in this case so that insurance companies would actually pay for it for a new indication but given the size and the number of these studies you're also looking for hey is there another side effect here that we've missed in the phase three right and it might be the the particular population they might have a different risk profile so that a you might you might have a different threshold that's right because you're not doing this in patients with type 2 diabetes you're doing this in patients who explicitly don't have diabetes but have obesity ah you know different patients are we going to see something different here so yeah so um anyway that's that's the that's the long and short of phases one two three and four okay so going back to observational studies are there any things that you look for in particular that that will increase or decrease your confidence in it whether that it's a pearl necklace or or garbage yeah I mean I and you've already kind of touched on this but I I think that um you know selection bias is is is a big one uh so so I think when I think about observational studies whether they be prospective or um retrospective the healthy user bias I think is one of the more common ones we see in the epidemiology as it pertains to health so

[00:29:09] I wouldn't even know where to begin talking about these studies because the examples are so Myriad but you know is bacon bad for you well if you look at observational epidemiology bacon is almost always bad for you I don't know what the hazard ratios are Bob but it's it's probably in the neighborhood of 1.3 or something like that meaning it has about a 30 percent increase in the risk of basically anything you look at right whether it be cancer heart disease death is that directionally right I I think that's right I mean there's probably more Nuance because they're you know it's the who is looking at I think they said over 700 epidemiological studies for red meat consumption and I think they also had processed meat consumption when you look at those you know we can get into it but how are they measuring bacon consumption you know they're using these food frequency questionnaires probably get into this recall bias um but yeah generally I think I think with the Who stuff I think it was about 20 to 30 percent Associated increase yeah and so you look at that at the surface of course you'd be concerned you'd be like oh my God like I shouldn't be eating fill in the blank I shouldn't be drinking coffee I shouldn't be eating bacon I shouldn't be eating meat at all um the problem with these studies is that you can't ever no matter how much you try to statistically reconcile it you can't strip out the fact that people make choices not in isolation so is there any difference between a person who makes a lifelong decision to not eat meat and a person who doesn't of course there is and it's going to come down to many things that go beyond their diet including things that can't be controlled for now obviously you can control for some things smoking a person who doesn't eat meat is far less likely to smoke than a person who does a person who doesn't eat meat is probably far more likely to exercise or pay attention to their sleep habits or be more compliant with their medications or things like that again not the people who don't eat meat don't care about those things but people who don't eat meat pardon me not the people who eat me don't care about the things people who don't eat meat basically that is a proxy that is a really good marker for someone who is very very health conscious so this healthy user bias permeates everywhere and by the way it permeates in both directions so if you look at the epidemiology that started to become very popular about 10 years ago that was suggesting that diet soda was more fattening than soda right so drinking a diet coke is worse than drinking a Coke well on the surface that doesn't seem to make a lot of sense right I mean

[00:31:52] Diet Coke has no calories in it Coke is full of just liquid sugar and of course it gets you thinking oh is it the aspartame or whatever else well I mean a far simpler explanation is look at people who are drinking diet soda versus people who are drinking soda you could make an argument and I think this is the argument that as a person is becoming more metabolically ill and they're being informed that they really need to stop drinking soda they're going to be drinking diet soda and so it's very difficult to look at just people drink this people drink that they're otherwise identical and simply the only difference between them is what they drink it just it just doesn't really hold up so anyway you're always going to look for that that sort of healthy user bias you talked about another bias a second ago which is informational recall bias and I think many people are just shocked to learn how clunky and clergy nutritional epidemiology is I mean it's like when you think about all of the amazing technology we have in the world and you know you look at

[00:33:00] I mean we just recently uh did a podcast and you know talking about some of the most Cutting Edge uh tools of Neuroscience that allow you to examine the behavior of a single neuron using Channel options and all these things like that's at one end of Science and at the other end of science we have this thing called a food frequency questionnaire where you you get a call from from Billy and he asks you hey do you remember how many times a week you ate oatmeal for the past year

[00:33:32] I mean I I pay quite a bit of attention to what I eat I don't know how I'd answer that question you just go to your spreadsheet of your oatmeal consumption right yeah now you've dug into this a bit Bob can you I'm being a bit tongue-in-cheek and facetious can you try to make the case that that recall bias isn't really that bad and I'm just exaggerating uh I can't make that case I guess it might depend on you know what are you recalling okay yeah what's the what's the best case scenario best case scenario well maybe so I would if I would probably get out of the food category it might have to do with something say like smoking history and you might you might even have receipts of the last time that you you paid for cigarettes or something like that that if you ask people how much did you smoke in the last year I think you can get a more accurate answer but with the food frequency questionnaires I think I mean there's so many analyzes but even just the number of foods that are out there compared to the number of foods that are that are encapsulated in the food frequency questionnaire vastly different it only covers a very small portion of it but and it's actually the foods that that I think the epidemiologists often look at like the the red meat consumption and things like that the people will um vastly under the is it they they will underestimate it's when they do like these validity studies and actually follow them or they do like a food log compared to their food frequency compared to the food frequency questionnaires it's the correlation is so low that it's it's so underestimated that you're not really getting an accurate picture so um I don't know about like a best case scenario with food frequency questionnaires for food I guess it would depend on it would be on frequency imagine if you got a food frequency questionnaire that was uh maybe more technologically advanced where you got like a it's an app where you literally recall at the end of each day what did I eat today or even after each meal but the problem is I think is the frequency of this that oftentimes it's it's it you're you're doing one questionnaire for what did you eat over the course of say uh one year or two years or even they'll just do one food frequency questionnaire at your at the beginning of the study at Baseline they'll follow up with these people for say say 10 years 20 years and the assumption is they don't change their eating habits they never ask them again and you know what happened here but if they look at you know they might compare two groups one group has higher bacon consumption than the other the assumption is is that they're going to continue with those dietary habits for you know in perpetuity um so again that's not a best case by a scenario but I guess the best case scenario is that the you could have more rigor I think if you did it more frequently because obviously I think you know I if I asked you what you had for breakfast this morning I think you probably have more confidence in the answer than what did you have for breakfast on January uh third of of last year yeah I mean I think with nutrition I just because I spend so much time doing this type of stuff with patients uh it's it's metaphysically impossible and I re

[00:36:31] I mean I really feel strongly that we should abandon food frequency questionnaires and no study should ever be published that includes them that's just that's gonna I'm gonna anger a number of epidemiologists listening to this but I I really think we need to put a stop to that I think where recall is reasonable is as you said on things that are more profound I mean if we wanted to do a study on the uh I think of something really that you would never forget like oh childbirth like asking women to recall how many times have you been pregnant how many times did you either have an abortion or miscarry and how many times did you deliver it term like something that profound yeah I would feel confident that if you asked a woman that question over the past 10 years of her life you would get very accurate answers but by the way it still doesn't tell me that I would be able to infer causality if I was trying to look at women who have you know never had a miscarriage versus women who have had miscarriages just because I look back and ask them to tell me those things doesn't mean that embedded within those differences are other biological or social or economic factors so you kind of get where we're going here which is you know I think epidemiology has a place but I think it's the pendulum has sort of swung a little too far and it's it's it's its place has been asserted as being more valuable than I think it probably is um you you kind of talked about something that's um I think a very important bias that exists in any study but I think this is actually a big problem in in experimental studies in in prospective studies if they're done incorrectly which is something called performance bias so the Hawthorne effect is is basically an effect that says if a person is watching you you will change your behavior so anybody who has tried to fastidiously log what they eat every day which I've done many people have done this there's no question you change your behavior just by logging what you eat you will change what you eat how much more will you do it when you know someone is going to look at it unbelievably so in fact you could make a case that one of the most efficacious dietary interventions known to man is having somebody watch what you eat every meal not just every meal every moment like and whether you have somebody virtually or literally watching you at every moment eat um especially someone who you know you're not entirely comfortable with that's going to have an enormous impact

[00:39:18] I think I think isn't this isn't there a name for this that's like a car it's like is it the Hertz effect yeah there's a it's the Avis effect because actually I think Hearts might in the 70s and 80s Avis was always behind oh they were behind it was a budget yeah it hurts their budget Budget rent a car so though so their slogan I thought it was a great you know kind of interesting basically we're number two and then they would say we try harder because like because remember it's inferiority complex we're number two and that I'm trying to think of an example because that the Hawthorne effect is you know it's almost like a experimenter bias that the experimenter is watching the observing the The People

[00:39:57] Under the lamp that was the where this came from and looked at work productivity with different different lighting um they got you know they got the clipboard and it could be your boss that's out there and looking and watch watching you and so that experimenter is having the effect with the Avis with the

[00:40:11] Avis effect I think it's more trying to think of an example so say that you are say you Peter competitive Peter were enrolled in a in a like a cycling trial and you realize say it's open label and you get a you know you get a placebo or you get nothing and you know that there's another group out there that say

[00:40:30] I think we looked at this a little bit say it's like this lotion that you put on and supposedly it's supposed to improve your performance you know there could be a part of you that says like I'm gonna beat those guys you know the control group they're gonna say like I yeah we're number two like we're not getting the special treatment um so we're going to win this thing you know it sounds like that wouldn't happen but I think that people if they enroll in a trial sometimes there's that competitive nature and it's also to your point about um if you have somebody watching you that that could also uh adjust adjust your performance now you have this whether it's not physically somebody watching you but you know that there's a trial that's following you and that you know that they're going to be looking you know a year down the road two years down the road or even you know three weeks down the road is probably more common and they're gonna they're gonna test you again and see how you do and see if you improve or see if you don't um those things can play a role and just to to the to the point of somebody that you know might be not pleasant that's watching you as far as uh food there's a there's a great Saturday Saturday Night

[00:41:26] Live clip with it's the rock in the clip and it's a commercial for it's called Uh nicotrel and it's a smoking cessation treatment and it's actually the rock is named Nick catrell and the guys there's a guy on a couch and he's about to smoke a cigarette and you know the rock comes out jacked smack you know smack some smacks a cigarette out of his hand and and it's one of the most effective smoking cessation programs I've seen

[00:41:53] I bet I think there's a more um Sinister form of performance bias that creeps up in clinical trials uh especially in randomized control trials where you think at the surface wow this is a really well done study so you'll take two groups and let's say it's a weight loss trial and we're going to test calorie restriction versus uh the pick your diet your your favorite the all potato diet um so the calorie restricted group is given some leaflets and it tells them how to measure calories and that they need to cut their calories by 25 from

[00:42:35] Baseline and we'll see you in 12 weeks the potato diet group is uh given twice weekly counseling sessions on all the different ways you can cook potatoes so that you don't get you know Fed Up and bored of eating potatoes all day on the potato diet group and at the end of the study the potato diet group lost more weight than the calorie restricted group well it'd be tempting to say well come on this is a randomized controlled trial

[00:43:03] I mean like it but but the problem is there's an enormous performance bias in the potato group in that they were given far more attention they were observed more they were given more coaching they had much more of a positive behavioral influence um and I I would say that's the number one bias that I see in rcts that are lifestyle based is is that very sort of subtle performance bias so you if you're really designing a trial well you have to flatten the curve on those differences so each person in each group should be getting the exact same amount of attention uh the exact same amount of touch with the investigators the exact same type of advice uh so that you can sort of eliminate that that that difference which unfortunately it shows up a lot yeah I think that that that almost gets back to this idea the null hypothesis that uh would you say coffee coffee might darken your eyes and that that's your hypo that's your guess maybe you've observed it you've got a couple of case studies of some people in your family or whatever and so that's your hypothesis and then the way that you design the trial is interesting because it's really like

[00:44:19] Kathy this coffee is going to be innocent until proven guilty that's going to be the the default position and then it's it's really your role and it seems almost counterintuitive to a lot of people and it's hard actually from a human perspective is that your role is really to be as rigorous as possible to to essentially uh you know falsify your hypothesis you need to do that you know as rigorously as you can and sometimes I think to your point sometimes it's like you get really excited about a treatment the people that are involved in the study the investigators they're really excited about it and the the control group or the placebo it's almost an afterthought and so there might be a lot of things that they're they're doing in the treatment group not just the treatment itself that could bias the biases study yeah yeah um so so kind of continuing on that threat of other things that you want to look at in a study is and we talked about this a little very briefly in passing was the idea of differentiating primary from secondary outcomes so um and there's some debate about whether you can only have one primary outcome or whether you can have co-primary outcomes but the primary outcomes are basically the outcomes for which the study is designed around and powered against again we will come to this idea of power in a moment um but there are lots of secondary outcomes and they're often exploratory so it's really important that when people are pre-registering studies they

[00:45:38] State what the primary outcome is and what the secondary outcomes are and typically a study that fails to meet its primary outcome will be deemed a null study even if it meets secondary outcomes so it's just very important to pay attention to to the subtlety of that and again a good journal with a pre-registered study is going to make that abundantly clear but I can promise you that uh you know someone writing about it in the newspaper is virtually never going to make that distinction and it's important to understand that because it gets to this next issue which is kind of the multiple hypothesis testing problem so um you know research should be hypothesis seeking uh or or or you know hypothesis testing but it can also be hypothesis generating and so you can use statistical tools to slice and dice data in multiple ways and you can take many looks at data to see if you actually find something significant there you have to be careful because the more you look the more times you look at something the more likely you are are to find something that is indeed positive so um

[00:46:49] I mean this isn't a great analogy but just to kind of give you a sense of it if you if you flip a coin and it's a fair coin you've got like a 50 chance of getting heads uh if you get two chances to flip the coin the probability that you're going to get heads is now 75 if you get three chances to flip a coin you're up to you know 87 and a half percent chance that you're going to get at least one head um if you flip this thing up you know 10 times you're basically at a hundred percent uh likely that you're going to get heads so if you're allowed 10 looks you have to correct for that and then there's something in statistics called a boniferoni correction factor that that does force you to do that it forces you to divide your p-value by n where n is the number of times you've taken a look at the data so to speak and therefore it raises the bar for what is significant and again we'll we'll talk about p-values for folks who maybe aren't as familiar with that in a second um anything else I mean I'm kind of rattling off a long list of things that are one one would look for is there anything else that you'd add that I'm sure I'm missing some things well I think it's maybe a more technical term that we didn't bring up which is confounding but that when we talked about the healthy user bias I think that's a that's a great example of something that can confound confound your results that that doesn't necess it's not in the causal pathway let's call it that that might be affecting the results whether it's age sex smoking I mean the the list is almost endless and this is what this is what those observational studies will try to control for um in order in order to almost mimic what randomization would look like right this is this is the sort of debane of the existence of the epidemiologist right so if if um if if you're trying to determine a relationship between hot chocolate consumption and skiing accidents uh it's very likely that people who drink more hot chocolate are more likely to have ski accidents um so this I mean the skiing cause hot chocolate consumption or ski accidents cause hot chocolate consumption does consuming hot chocolate make you a worse skier um you know or is it that people who live in colder climates consume more hot chocolate and usually skiing occurs in colder climates outside of Dubai so um climate therefore is obviously a confounder and the goal is to be able to identify every possible confounder when you're doing epidemiology and I I think as John ionitis argued uh when we had him on our podcast and that's that would be a good podcast for people to go back and listen to alongside this it's really not possible to identify let alone eliminate all confounders absolutely so if we look at experiments experiments or experimental studies compared to observational studies are there things you look for specifically or in particular for experimental studies to increase or decrease your confidence yeah well first and foremost randomization right so if an experiment isn't randomized again it doesn't mean that it's useless but it just means it's going to be a lot harder to really make sense of this and randomization needs to be um a rigorous randomization uh you you can you can randomize incorrectly believe it or not I think there's a very famous example with predomed which was a study that when it was published was kind of a remarkable finding a very large study something like 7 500 people randomized into three groups 2500 per group uh given basically two different dietary patterns a Mediterranean diet in two versions in a low-fat diet this is a primary prevention study so it was looking at people who are high risk but who haven't had heart attacks or anything yet and it was looking at mortality and the study was actually stopped early again something we're going to talk about in a second because it had such a positive effect so the Mediterranean diet had such a favorable effect relative to the low-fat diet that people were dying at a rate far less such that it would have been unethical to continue the study for the I think the seven and a half years it was planned to run and I think they stopped it in the four-year

[00:50:54] Mark and sort of declared victory but then something happened Bob what what happened actually so they went they went back and reanalyzed the the this predimid group um the first paper was published in New England Journal medicine 2013 and they had a basically a correction but they they almost like a brand new article um addressing some issues so they did they did a re-analysis that was published in 2018. I think it came from this this fellow named John Carlisle who had this way this this way of looking and I think this was that we have a we've got a email on this with David Allison just a great statistician he talks about this in his article too um as well where he he looked at this but um this fellow named John Carlisle did this analysis where he looked at thousands of studies and he could flag the studies and see does this does this truly look like randomization based on some particular statistics and the predimate study was flagged looking like this doesn't look like proper randomization it might be something going on here and I think according to the you know the the media Outlets I think I read in the New York Times they talked to the the lead or the senior investigator and and he said that it turns out that um some of the villages or the the clinics

[00:52:11] I forget how many clinics in total there there were in the study but at 11 of the clinics they one of the uh investigators were randomizing the entire clinics um to one group and that this is what I think is really interesting about just looking at like if you really want to dig into a study sometimes you really have to get the story which is and and why you think it I think oftentimes you look at randomization oh that's really simple you just randomize people to different groups and you know blinded or unblinded it's very it's very hard to

[00:52:40] Blind uh the fact that you you see your you see your neighbor get a delivery every week of all of a jug of olive oil or a or a sack of uh mixed nuts which were the two Mediterranean groups and I think what happened was people started complaining in the in the in the Villages they're like what do I get and they're like you got your you got your low-fat diet pamphlet remember like we give it to you every year um so that happened been in that study and that that's typically referred to as a so you can do that in a study and that's typically referred to as a as a cluster a cluster randomization where you might you might randomize one classroom to another classroom which might be convenient but it it requires different statistical methods so that was one of the things that happened there and then they also another one let's use that example because that's actually a really good one right if you want to study the effects of meditation on you know attention span of kids it's very different to say we're going to just take a hundred kids and randomize 50 into one group 50 into another and separate them versus saying we've got a class two classes over here two classes over here we're going to split those two and two into the effect that's a totally different type of randomization one is a true randomization one's a cluster randomization and while you can do the latter it requires a different statistical adjustment so yeah I think predom had basically had to re-analyze all of their data in light of that it it turned out in the case of predomen the results still held um but it will always kind of be a cloud that hangs over it yeah and I think you need to you know just to make that his point when he found out he I think he he was a huge fan of the predimid study and something that he said which I think might be intuitive is he said if they're randomizing entire Villages you know to to a group and they're not they're not accounting for it he thinks like I'm not sure that's going to be the only problem in that study and that every you know everything was uncovered but yeah but on the flip side it's really really hard to do everything everything right in a study you're going to make mistakes yeah and now imagine randomizing a household where you say okay you know Dad you're on a Mediterranean diet for the next seven years mom you're on a low-fat diet for the next seven years I mean it starts to get very difficult um okay so again that's one important thing you also want to make sure is there a control group um not all random design not all um prospective trials have control groups sometimes it's a single group where a person serves as their own control and there's typically a crossover so you'll take a group you'll randomize them into two but there's um it's not that one group is getting treatment a and the other group is getting Placebo or treatment B it's both groups get both treatments plus or minus a placebo in different orders and this is a great statistical tool provided the treatment doesn't interfere with the washout or the the sort of the treatment doesn't interfere with the control session the reason this is powerful is you you need far fewer subjects when everybody gets to serve as their own control so it greatly reduces the you know basically the cost and

[00:56:04] Logistics of a study but you run into challenges right so if you take 20 people are going to take this drug that is supposed to you know help them exercise better for eight weeks and another group is going to take a placebo for eight weeks and exercise and then everybody switches because that's the right way you would do it you had some people start first on the treatment some people start first in the placebo you have to figure out is there do you need a gap between the treatments because will the effects of that drug linger into the placebo period for one group which is not what's happening to the other group and even if it is um even if you're only doing it with one group are you confounding the effect of that treatment I hope that makes sense

[00:56:49] Bob I don't know if I'm making sense I know you know what I'm saying but is there a better way to explain that I think that makes sense one other point I was going to make about that too with the crossover group so it's going to ask ask you about that because I've seen the the statistical power I guess you would call it the crossover groups as you can see relatively small studies not a lot of people pretty short and you look at the P values and we'll get into that but they're you know 0.000 you know something and I I guess like the

[00:57:15] Assumption when I was thinking about it when you're talking about it and you know they serve as their own controls it's almost as if they're treating them like if you could get identical twins and randomize those identical twins to one group or the other you would think that's great because you're controlling for so many things um about you know the physiology or the the the genotype Etc about those people and it's almost like they treat these crossover groups is that you're you're you're almost cloning these people you're comparing them to themselves but

[00:57:42] I think you it's a it's a good point and there might be something about the order of of the the treatments that they receive if they get you know treatment a and then treatment B maybe you know one might have an effect on the other and the really good ones go a b and then ba they divide them into two groups and go a b and b a and yeah it really comes down to the fact that you can use what's called a paired t-test um and and the the the Simplicity of the statistic of the paired t-test is is part of its Elegance here and that it it uh it basically eliminates a lot of variance um okay so then we talked earlier about this blinding what does that mean so in an Ideal World both the subjects and the investigators should not know who is getting the treatment and who is getting the placebo at a minimum the subjects should not know that would be single blinding but again double blinding is always preferred if possible because the investigators can be biased they can have hidden biases if they know the outcome so for example if patients are being given a drug for weight loss you could say well it's pretty easy to you know blind the patients from that but if the investigators know that they might behave differently towards the patients for whom they expect greater weight loss if they believe that this drug is effective so again very important and sometimes very challenging you know I think we talked about this in the podcast with Rick doblin one of the huge challenges of studying psychedelics is it's very difficult to Blind anybody most of all the user right the subject you know when you one group is getting psilocybin and the other group is getting even if it's niacin which causes some flushing you know it's it's it's not hard to know which group you're in and that may affect the results um Size Matters duration matters um and and basically the uh the generalizable generalizability of the study so is it in a population that replicates or looks like what I'm interested in studying whether it's me or my patient or whomever I I care about and there are strengths and weaknesses to mass heterogeneity of studies so the more heterogeneous a study in terms of its patient population well the more generalizable the results are but the higher and the higher the bar for finding it so one of the big uh

[01:00:16] I think this is I think this has got a lot of attention lately but I think for a while it was a relatively unknown kind of dirty little secretive medicine was how many clinical trials involved men only and how many drugs were approved for both men and women but on the basis of only being studied in men and the rationale for this was that it was more complicated to study women right so women especially premenopausal women because they have a menstrual cycle that really changes things hormonally and therefore it's more complicated to do studies and look at drug kinetics and all sorts of things in women and so the easier way to do that was to just study it in a homogeneous population of men well of course that poses an enormous problem if you're now trying to extrapolate the utility of that drug in women so it's an extreme example but a very important one um for large studies you you tend to want to know is this is this a multi-site or a single site again predimet's a great example right so you had a multi-site study and there were probably significant differences between how the sites were run so there's an advantage to multi-sites because in theory it brings more heterogeneity it should cancel out the effect of any one study over another but it's harder to control and therefore you can have whether it be deliberately or non-deliberately Rogue studies or sites rather introducing more more bias um I think another thing I really look at here is how big is the association of the effect um and we'll talk about this with power but you can have something that is statistically significant so in that sense the study is quote unquote a success but it's clinically irrelevant the effect is not that big so we we've tested this new drug for blood pressure and it lowers systolic blood pressure by one millimeter of mercury after a year of use and it's like okay that might be statistically significant if the study was large enough is it clinically significant almost assuredly not um you want to pay attention to what the

[01:02:21] Adverse Events were both in frequency severity and distribution um you want to pay very close attention to who funded the trial uh trials don't fund themselves and uh a lot of Trials are funded by drug companies now again they're usually done with um very clear data monitoring and data analytics and despite all of the sort of fear-mongering out there it's not like farmer really gets to put their hand on the scale of these Pharma studies but where I think things can get a little dicey is in terms of you know things getting buried in supplemental journals and things like that so so you do want to pay a bit of attention to who's who's funding a trial and and I think even more important than that is kind of understanding what the conflicts of interest are of the authors and and nowadays those have to be declared but there's been a huge amount of hoopla over that and there have been some very famous examples of um people who are on editorial Boards of journals or publishing like crazy and not declaring that hey I'm a paid consultant of these 10 Pharma companies companies and I'm writing or doing experiments on drugs by these people or

[01:03:36] I'm an editor on journals that are commenting on on this and then finally you you really want to understand if the study was adequately powered and this becomes very important if the study has a null outcome so you want to just spend a minute we'll talk about power yeah I think that that makes sense so power is um defined as 1 minus beta where beta is defined as the probability of a false negative now let's contrast that for a moment by talking about what a false positive is a false positive is defined as Alpha and that's also known as the p-value okay so so let me kind of restate this so so a p-value is basically trying to answer the question what's the probability of rejecting the null hypothesis when it is in fact true so if the p-value is zero it means it's impossible and if it's one it means you are absolutely going to do it so obviously we want P values that are as small as possible it can never be zero but you want them to be as close to zero as possible and basically we say five percent is our minimum threshold uh or really our maximum threshold that's the ceiling that we'll put on this idea so um you go back to what we talked about the outset so the default position is uh that the that the null hypothesis is correct that there is no difference between the groups so um this term statistical significance basically means that the null hypothesis is rejected if the p-value is less than that pre-stated level um

[01:05:25] I don't know if I'm explaining this really really well Bob is there anything you would add to this because I think this is an important idea even though it's p-values are so ubiquitous but I I think it's maybe worth spending one more minute on it before we go back to Power yeah I mean to me it sounds like it makes sense I'm trying to think of a of somebody who might not understand it as well but I think

[01:05:49] I think that those examples that you gave are good that that it and this is so you see on most papers I think you'll you'll see this p-value of 0.05 and we can get into the confidence intervals but you'll see 95 percent confidence interval um and p-value of 0.05 and like you said that that's your false positive rate so I I guess maybe imagine and it's an arbitrary threshold so you could try to submit a paper and I I've seen sometimes it's I usually catch it by the confidence interval I'll see 90 confidence interval on some table and also there are some figure or table and

[01:06:23] I'll look at it and it'll they'll use a p-value of of less than 0.1 for and maybe they have some justification for it or or not but it really is this arbitrary threshold so I think like imagine if your p-value was uh you're gonna if it's less than you know 0.95 we're gonna we're gonna reject the null hypothesis so you could have a you know in theory if if this thing if not if if if the you know it's not exact but if it's the chance of this being a false positive is about 90 based on your based on your analysis um that's still you you would you would reject the null hypothesis if there's no difference between these groups which sounds sort of insane so it's it's kind of this I don't know I think it was this guy Fisher who maybe has established this 0.05 but this has been the threshold that they're they're more or less they're willing to accept at least for the purposes of a single trial that the p-value of 0.05 that they're willing to accept a level of of a false positive in their results and still make that claim that they they rejected that hypothesis right because if you make the p-value so low if you say no my threshold is .0001 well then you really run the risk of discarding a lot of information that turns out to be kind of relevant so um you know it is a fine balance between those two so so now that would be that would be a false negative right so exactly the lower there yeah so yeah there might be an effect but you're not going to see it so so this false negative rate um we typically allow to be a larger number it's typically between 10 and 20 percent and what that means is the the the the the the flip side of that is we have 80 to 90 percent power because one minus that accepted false negative rate is called your power and

[01:08:18] I you know I I think this is one of the most important Concepts to understand in designing any sort of clinical trial whether it's humans animals any sort of intervention so um there's a there's a table uh they're all over the place but this is the one I've always liked it's old it's probably 10 years old probably longer than that actually but it's out of an old great uh textbook uh cancer textbook on clinical trials so so pull up pull up this table

[01:08:43] Bob and we'll kind of walk out okay got it power table okay all right these look a little intimidating at the outset so let's kind of walk through um how to interpret this so what this table is saying is you want to pre-suppose you know what the difference is between the treatment groups you you have to say

[01:09:11] I believe that the difference between the success rate and the treatment between group a and Group B is going to be X percent and the smaller of the two is y percent so let's come up with a real number so I think that we are going to look at how this drug um you know impacts your rate of surviving a urinary tract infection or being you know cured of this infection and I think that the placebo group is going to have a success rate of 25 percent and I think that the treatment group is going to have a success of 35 percent so I think there's a 10 Gap and I think the lower of those two is 25 percent so you go to 0.25 on the horizontal axis um and you go to 0.1 over on the the column and you'll see there's two numbers there 459 and 358. and the upper of those two is if you want 90 power I.E 10 false negative and the lower of those two is for 80 percent power or 20 false negative rate and those numbers basically tell you how many people you need in each of the two treatment groups if you want to be significant at a level of 0.05 percent so what do you notice when you look at this you notice that the bigger the Gap the bigger the effect size between the two groups the fewer subjects you need so if you march from right or pardon me from left to right in this table holding that effect size at 0.25 if you say well the difference is 15 percent it goes to you only need 216 or 165. if the difference is 30 percent so one group is going to have a 25 success rate one group's gonna have a 55 success rate you're down to 60 and 47 and if you go out to a 50 difference so one group is going to have a 25 response rate the other group's a 75 response rate you're now down to needing somewhere between 18 and 23 people per arm and by the way if you go down to five percent one group responds at 25 percent the other at 30 percent you're at 1700 or nearly thirteen hundred depending on your level of power so I appreciate everybody kind of bearing with me as I went through this power table it seems like one of the driest things in the world but but as my mentor once told me it's the single most important table you should ever familiarize yourself with if you want to be in the business of Designing clinical trials or basically any sort of experiment because it is just so easy to get this wrong and over or under power an experiment so what does that mean so to underpower experiment I think is the more common mistake here so you simply don't have enough people in the study to appreciate a difference if it is there and so the study ends up being null the p-value does not exceed the threshold of 0.05 and you say look there is no difference between treatment a and treatment B when in reality there may well have been but you didn't have the power to determine it and therefore you don't actually know if you should have rejected the null hypothesis or accepted it

[01:13:03] I think that makes sense right Bob does that yeah that yeah I think the other problem equally Sinister perhaps not as common is when a study is overpowered and now you have more people in the study than you should have had for the effect size and you start to find things that are statistically significant but are probably irrelevant clinically so you that's when you start to pick up an effect size of one percent when you're dealing with something clinically that should never be thought of as being relevant below 10 detection threshold so notwithstanding the fact that you also probably you know had more people in a study than you needed to it could have cost more and you typically don't see this as much with clinical trials but you'll see this more with kind of data dump trials um so so sort of data mining studies where they're you know grossly overpowered um okay I kind of got a way off on a tangent there I don't know why I went down that path of power but um I know it's important so I think we got on the subject uh because we were looking at things you look for in experimental study that increase or decrease your confidence in it and I think it's something that's if people have this list it's it's often left off I think it's important yep okay good so yeah power power matters and and when you look at a study and it's not significant you should ask the question was this study powered correctly and

[01:14:31] I mean I can't tell just by looking I actually have to pull out that table we just went over and and go through the Matrix and go okay well you know this is how many people were in it therefore at 80 power they were detected to tell a difference between the two groups of this much with an effect size here and and and then a lot of times like oh wow this actually study this this study wasn't powered appropriately anyway so I I've learned nothing new here unfortunately is it true that you have a laminate of this in your wallet this power table

[01:15:01] I don't anymore but I used to have a laminated copy at my desk yes I made placemats out of it for the kids they they love it very nice oh hours of enjoyment um so related to this I think you probably without looking at their you know their their power analysis which often I think this is like a maybe a tip is often you won't see anything in their paper but you might see it in the in the protocol if they include that where they'll they'll talk about how they powered the study what was their justification what was the effect size that they were looking for and how many how many participants did they did they need and and then you can look at how many they actually got in the trial that actually completed the trial or or enrolled in the trial and to your I think to your point of overpowering a study sometimes you could you might be able to discern it if you're looking at that example or I think you said there's a drug that that lowers systolic blood pressure by uh by one millimeters of mercury and the results are statistically significant I think that that that might put up your feelers and say you know how many thousands of patients were in this study and and um I think that that that gets to another question which is looking at how how these differences are actually determined whether when you're looking at when you're looking at the say the the effect in one group versus the effect in other in another group so what are some of the ways in which researchers measure the association or the quote unquote effect size in these studies well a lot of times it's only reported as a relative risk but I I think you and I have harped on this in the past which is you can't really talk about relative risk without knowing absolute risk and sometimes they don't give you enough data in the paper to do that and it's infuriating actually but absolute risk is um let's use the example right it's sort of like you know group one had at the end of this study of five percent risk of uh dying and the other group had a three percent risk of dying so what's the absolute risk it's five percent in one group three percent in another so therefore we have What's called the ARR or the absolute risk reduction is the Delta between those two so the

[01:17:16] ARR is five percent minus three percent is two percent there was a two percent absolute risk reduction um and that's important to know because often what's only reported is the relative risk reduction which is the absolute risk reduction over the non-exposure absolute risk so in this case the relative risk reduction would be whatever the absolute one was I think two percent divided by the non-exposure risk which is five percent so that's 40 so they had a 40 percent relative risk reduction going from five percent to three percent so both of those things are important but again it's really critical that you know both one of my favorite examples of this of course is the famous women's health initiative which was looking at the increase in the risk of breast cancer for the women who were receiving the um estrogen and synthetic progesterone treatment now notwithstanding the fact that I and we've talked about this a hundred times why I don't think that that study was a good study in any way shape or form and

[01:18:34] I don't think that the study demonstrated there was any difference in Risk statistically here's what got reported it got reported that the women receiving the hormone replacement therapy had a 25 increase in breast cancer and that was true at a relative risk level but the absolute risk difference was a difference of five women per thousand to

[01:19:00] Four Women per thousand so as you went from four cases of breast cancer per thousand women to five cases of breast cancer per thousand women that is indeed an increase of 25 5 minus four is one divided by 4 is 0.25 but what's the absolute risk reduction or in this case risk increase it's one over a thousand or zero point one percent so you know what I usually say to women when we're talking about hormone replacement therapy is you can kind of use that as your ceiling for the true risk increase of this therapy even if you discount the 12 mistakes in that study that make it hard to believe that that effect size would hold so another way that we tend to measure effect size or Association is using something called a hazard ratio um Hazard ratio actually involves some really complicated math that we're not going to get into something called the

[01:19:56] Cox proportional Hazard which I'm embarrassed to say I don't actually know the math anymore there was a day when I did and I remember it was not easy for me to learn um I had to go out and buy a bunch of um books on statistics because even though my background's in math I did not have a huge background in stats and um it wasn't like rocket science but I remember really having to to sort of understand the mathematics behind the

[01:20:21] Cox proportional Hazard the magic of the hazard ratio is it is temporal so it captures the risk of something I.E the hazard over time and that differentiates it from something called an odds ratio which can't do that which only can measure kind of over the entire period of time what is the risk so um uh at the risk of oversimplifying this a little bit let's talk about the hazard ratio over a given period of time but acknowledging that it's it's real magic is its ability to tell you what's happening at any point in time so um let's just pretend we're talking about a cancer drug trial and um the hazard rates I.E the rates of disease progression were 20 in one group and 30 in another group so the people getting the drug progressed 20 of the time the people not getting the drug they're getting the placebo progressed 30 percent of the time so the hazard ratio is the ratio of 0.2 to 0.3 which is 0.667 so in other words they're um the the treatment group was 67 percent as likely to experience disease progression as the control group um so now again you could flip the math and say well what if you saw the exact same rates but in something that was desirable so then it would be the 0.3 over the 0.2 would be 1.5 so your hazard ratio would be 0.15 which means there's a 50 percent increase in the benefit or the harm if it's something that's harmful so you can look at the single number of a hazard ratio and you can determine what the increase or decrease in Risk was so so Bob I'll quiz you and you you tell tell me in the and The

[01:22:11] Listener how you're figuring this out the hazard ratio is .82 0.82 so let's say that it's you're comparing the experiment to the control or the experimental group to the control the experimental group is uh has a I would probably flip it I would say about 18 reduced risk of whatever the event is you're talking about very good progression all right all right so give me one Bob uh how about we'll go the other way the other side of the Ledger the other side of one a hazard ratio of 2.2 okay well that's a that's a tricky one let's if we said 1.8 it would be an 80 increase 2.2 would be a hundred and twenty percent increase how are you doing that you're taking 2.2 you're subtracting one and you get 1.2 and you multiply by a hundred percent and what you did earlier was when I gave you 0.82 you took one minus 0.82 and you got negative 0.18 which is a reduction of 18 so again you can just play with these for like five minutes it's it's actually not that complicated um but you just have to do a bunch of them and become familiar with what those numbers mean um now let's bring it back to the um ARR thing there's another common theme you'll hear about in Trials called the number needed to treat or the nnt analysis and this gets back to the importance of absolute risk reduction so remember the uh let's say there's an example of um well let's use the same numbers we used earlier just because they're familiar to me but you've got um a a drug that the people who take it have four heart attacks per thousand people over a five year period and then there's another and then the placebo they have five events over that same period of time per thousand people right so the the drug reduces the events from five out of a thousand to four out of a thousand so what's the relative risk reduction there the relative risk reduction is 20 right that's the four minus five divided by five in this case is a twenty percent relative risk reduction so you might say this is something we should be putting in the drinking water this is such an important thing right but you want to calculate how many people do you need to treat to prevent the event and to do that you have to take one and divide it by the absolute risk reduction not the relative risk reduction and the absolute risk reduction here is 0.01 percent and one divided by 0.01 percent is one thousand so now you have to treat a thousand people to achieve the effect which means you better figure out what the side effects are of that thing what the cost of that thing is what the complexity of it to justify there may be certain things for which a nnt of a thousand is valuable but you wouldn't say that across the board so um you know conversely if you have a drug that reduces the risk of death from four percent to two percent or say four percent to three percent then you would say 4 minus three is one percent one divided by one percent is a hundred if it took it from four to two percent it would be one divided by two percent is fifty if it went from four percent to one percent of reduction of death from four percent to one percent your difference is three percent you're now talking about an nnt of 33. as a general rule we love to see you know drugs in that sub 100 range of nnt um we tend to not get that impressed when the nnt of something is like a thousand so again that's another way to think about the effect size okay that and that makes that I'd like the number needed to treat because it really from a clinician's perspective from a practical perspective it's really telling it embedded in there obviously is the absolute risk and not just relative risk um so we went over P values and confidence intervals a little I don't think we went over confidence intervals as much as p-values do you want to stop and talk about confidence intervals sure why don't you take this one I need a drink by the way non-alcoholic just for those listening what do you drink again

[01:26:59] I'm drinking my Ghia with Topo Chico so confidence intervals are you know technically intervals in which the population statistic could lie so you've got typically I think what you see on a paper is this 95 CI it's usually abbreviated but it's a 95 confidence interval and it's usually reported next to the that that Hazard ratio that we just we just talked about so the Hat whether say the hazard ratio is uh zero the hazard ratio is 0.5 or yeah 0.5 which means basically uh a having of the risk say in the experimental group versus the control group and then you'll see this 95 confidence interval and it might say um it might give you this these two they'll give you these two numbers um and for example let's just say it's 0.2 to 1.2 is your confidence interval and the confidence that what that is is that's the flip side of the significance level which is 1 minus Alpha so it's 1 minus so we've talked about Alpha being the p-value but also being the the false positive rate so it's the flip side so when you see 0.05 for your B value that's a tip off that your confidence interval is 95 and I think a lot of people think about the word confidence um in this definition and they they take it to mean the probability that a specific confidence interval so my example I think of 0.4 to say 1.2 that that interval between those two numbers or between those two ratios contains the population parameter so they think they think okay we could be 95 confident that the true the true effect um of say meat consumption and cancer is between between these these two numbers um but that's not really what the confidence interval suggests it's really it's it's more of a suggestion I don't think this often happens in practice but if you were to take a hundred different samples and compute this confidence interval then approximately 95 out of those 100 uh will will contain the true mean value yeah I mean it's been described by some as an uncertainty interval rather than a confidence interval there's another way to do this was just kind of the quick and dirty way to do this is just to look at the confidence interval and ask if the interval contains one or not so um you you gave an example a second ago

[01:29:25] Bob you said your hazard ratio was what uh Hazard ratio is 0.5 0.5 okay with a confidence interval of 0.4 to 1.2 okay so that would not be significant so even though your hazard ratio you might look at that and say oh look that's a big reduction of 0.5 Hazard ratio means a 50 reduction but your confidence interval was very wide right it was all the way from 0.4 up to one point something so it crosses over Unity um conversely if you had a hazard ratio of 0.5 but your confidence interval was 0.4 to 0.6 or 0.7 or even up to 0.9 you would say indeed that is not at the level of 95 percent that is confident so the other thing you'll notice by the way is the closer one edge of the confidence interval comes to one the closer the p-value is to 0.05 so when you have like a confidence interval that runs from 1.01 up to two your p-value is probably about 0.049 or something like that whereas when you have confidence intervals that are miles away from one the p-values tend to be very small yeah and I think so that that statistic statistician Andrew Gilman he talked about uncertainty in intervals and the reason why he says that is Imagine you've got a huge confidence interval is what we call it so big confidence interval meaning instead of 0.4 to 1.2 it was like 40 reduction at 0.4 or it went it went all the way out to like say a thousand he would say like that's a huge uncertainty interval but the way that we think about confidence is oh that's a huge confidence interval and it's it's maybe intuitively backwards for some people to think about it that way yep absolutely agree um the the tighter the interval the more confidence you actually have in it and obviously it can't cross uncertainty that's right the less uncertainty there is and when you get these monster ones and this is why I like those sort of tornado graphs that you see in meta-analyzes where you visually get to see how much uncertainty existed in a given study now the confidence interval went so the hazard ratio is here's a great example Hazard ratio was 1.4 oh wow 40 increase the 95 confidence interval went from 1.1 to 17. well I mean do I really have a lot of confidence in that no that's an enormous uncertainty interval yeah and of course you would want to know okay what's deep what are we talking about here absolutely and not just relatively yeah yeah

[01:32:17] I mean look I think the takeaway of this entire section is if you make the decision that you want to pay attention to science you you just have to roll up your sleeves and accept the fact that you're you're not going to be able to read these things in the bathtub uh you know on a lazy Sunday morning like you you kind of have to roll up your sleeves and pay attention to all of this little stuff now it gets easier the more you do it it it it's you know I think when I read a paper today it's so much easier than it was 25 years ago um but you still have to kind of have your your guard up for all of these things yeah I think you learned something you might learn something new from virtually every paper you that you read so I'm sure you haven't but I you didn't read a paper 25 years ago and then you read your second paper today you've read a multitude of papers and in each one there's probably some something educational in there that you might pick up and that's that probably goes to I think the beginning of the beginning of the episode you're talking about I think

[01:33:17] Tim asking you know how do I get better at this and it's probably like is like be consistent repetition yeah every week yeah yeah um and you can see that some of the stuff we've talked about here I mean we just went kind of deep into some statistics and not even that deep right I mean we didn't really explain what the

[01:33:33] Cox proportional Hazard is and things like that and we didn't differentiate odds ratios imagine ratios which requires getting into more math but like you still have to be able to kind of crunch some numbers sometimes and um it's unfortunate that I think a lot of people in the media don't know how to do this and yet they're the ones that are reporting on these things so if you're if you're getting your science info from

[01:33:52] Twitter and from the news it's sort there's a little bit of a buyer beware um where you you you have to understand the fact that it's very likely that the people that are reporting these things not because they're necessarily uh not well-intentioned but they themselves might not be doing the type of analysis that's necessary yeah so another question we got is do studies ever stop Midway through if so what are the reasons yes they they do and they're they're generally kind of three reasons that studies are are stopped and again we're really talking about you know kind of prospective clinical trials here so um the first and most important of these is safety so remember we talked about phase one phase two phase three well phase one is all about safety phase two is about efficacy and safety phase three is really about Effectiveness and safety um but notice safety is in all of those so absolutely anytime there's a safety breach which means there is a statistically significant difference between an important safety metric in the between the groups that'll just stop the study the second thing that uh will stop a study is benefit so again the predomed example of when it was first done is it stopped two-thirds of the way through because it was deemed that there was such a benefit to the group on the

[01:35:21] Mediterranean diet relative to the low-fat diet that it would have been unethical to let those people in the low-fat diet continue for another two and a half years on a diet that was uh so clearly increasing their risk of mortality and then the final thing that will stop a study prematurely is futility this is a little bit harder to understand but it actually comes down to that Hazard ratio concept which is able to measure risk temporally and and sort of in an aggregate fashion so if two-thirds of the way through a study there's no benefit and statistically you know that nothing that's going to happen in the remainder of the study is going to change that you stop the study it's futile to continue the study so so so those are basically your big three reasons why a study is going to be stopped I think a good example of stopping a trial for safety was actually there's been several but one of the ctep

[01:36:18] Inhibitors of course a trap I don't know if I pronounced that correctly yes I remember this I I remember this really well this is one of the few moments in science where I remember where I was standing when the result was announced it was uh it was Q4 of 2006 I was at McKinsey at the time and I was walking up Kearney towards California Street and I heard the news of this and I couldn't believe I was so sure this was going to be a home run study yeah so in this case they the the trial was set up with 7 500 patients about in each group so they had a they were on a ctep inhibitor and they're all on statins they're on Lipitor in particular and so they compared the ctep inhibitor to just Lipitor alone just serves as a control group well it was it was actually the ctep inhibitor plus Lipitor versus Lipitor alone and this was a

[01:37:13] Pfizer study which everybody thought was very cheeky of Pfizer because Lipitor was about to come off patent and their way of sort of extending the life of it was saying hey when you pair the ctap inhibitor with Lipitor it's going to have a benefit because the the background on this is that ctep Inhibitors raised HDL cholesterol so it's like we're going to take a drug

[01:37:34] Lipitor that lowers LDL cholesterol we're going to pair it with a drug that raises HDL cholesterol how could this possibly go wrong famous last words yeah so they they intended to follow follow-up patients it's going to be about almost a five-year trial four and a half years and along the way they'll have a review board that's looking at the results of the study and in this case they had a monitoring board that was looking at in this case they're looking at death all-cause mortality and they they found that 82 82 patients receiving the drug combination had died compared with only 51 on

[01:38:11] Lipitor alone and so they advised Pfizer to Halt the trial at that point which it did immediately which it was just a little over a year into the trial um when they did it and it in a way it gets back to when you're talking about power analysis the way that they do this is they they have pre-specified p-values where they're they're kind of sneaking looks at the data like we talked about multiple hypothesis testing that they're actually taking a few shots on goal in a way because after 12 months they're going to actually compare the compare the two groups see if this thing is a p-value of less than 0.01 depending on what they're looking at and in this case they had this pre-specified p-value of less than 0.01 based on a test for death from any cause and they they found that and actually the paper there was still a published paper even though the trial only went for 12 months in the New England Journal of Medicine and they they report those those endpoints where the city was stopped yeah it's it's um another discussion in fact I feel like I feel like Tom Dayspring and I or

[01:39:14] Ron Krauss and I talked about this at length on on one of our podcasts I think it was I think it was time I did yeah that talked about this yeah yeah so I remember you telling that story just a quick follow-up question I guess hey was this the first ctep inhibitor that was was tested okay and then you had the follow-up ones and this this I don't think we've really addressed this but you know why would you do uh an observational study over a randomized controlled trial in some cases like you know a good example is try doing a random try getting it past the IRB uh a randomized controlled trial and you're gonna see you know get people a carton of Marlboros they're gonna smoke that over you know each day compared to a placebo group it's unethical and so with this it's I don't know how many times they saw these Adverse Events with each ctep inhibitor and it's I guess the same drug class they might have different mechanisms but does there does there become a point where where it's I guess maybe that's why they have the phase one phase two phase three and they get past those those barriers but in a way is it almost do you almost assume it might be unethical to run another ctep inhibitor trial if you're seeing you know differences in death the last however many times well I mean I don't think it's unethical because I think they are they are basically saying look it's a different drug and it's you know you can change one molecule uh on a drug and it completely changes the way it works uh you know you look at the cox-2

[01:40:31] Inhibitors you look at Celebrex versus viox I mean notwithstanding my views on that which I talk about with Eric tople on our podcast but um you know basically Two drugs nearly identical and one was far more efficacious but also had uh side effects and a subset of people with hypertension so um I think the real question is at what point do Pharma companies say enough is enough and I lost track I feel like there were three ctep Inhibitors that were brought to phase three and ultimately there was a mendelian randomization that looked at ctep mutations and and really found that this was not going to be a good strategy so um okay so that's a good example that you brought up with respect to [Music] um safety and then we talked about um benefit which was predimed and then uh I think what happened in the look ahead trial because I think lookahead was one that got stopped for futility right yes yep that was one where they they randomly assigned about 5 000 overweight or obese patients with type 2 diabetes and it was an intensive lifestyle intervention um and they were looking at um what was the the other that was the intervention group and then you had diabetes support and education in the control group and their their primary outcome what they were looking at is uh basically what's called mace so death from cardiovascular causes major adverse cardiovascular events um and I think the they were going to look for 13 and a half long trial almost 14 years and in this case the trial was stopped just under 10 years and it was based off of what what's called a futility analysis um that you explained yeah which basically means no matter what happens from this point on this study will not be significant so at the time that it was stopped the hazard ratio was 0.95 so there was a suggestion of a five percent reduction in in in the risk of death from cardiovascular events so in the right direction but the 95 confidence interval or uncertainty interval if we're going to adopt that terminology was 0.83 to 1.09 so it crossed one and so you know the p-value is going to be greater than 0.05 in fact the p-value was 0.5 0.51 or something like that I mean it was basically complete chance there was absolutely no effect and again no point in continuing okay so moving on to review process what is the review process once a study is done to get a paper published in a journal so you know once the once the study is done and they've done their analysis and they write up a manuscript they'll they'll submit it to a journal for publication and and that that journal will have an editor uh who will you know look to see if the you know paper meets their criteria um and if they think it's you know original and interesting right is this is this paper adding something to the body of knowledge um at that point the editor might just say you know hey this is not really a good fit for our journal or for whatever reason this is you know something we're not interested in any further you're free to go and submit this elsewhere um but otherwise the editor is going to invite individuals that are typically part of an editorial board to to peer review the manuscript so you hear this term all the time right which is is this a peer-reviewed publication and that's important because not all things that get published have been peer reviewed and that's obviously the highest standard so the um reviewers are basically invited not randomly but because they have some expertise in this area but other things are important right you know you have to consider the conflicts of interest um and that they might have to decline if they're conflicted and that's that's kind of a sticky topic because there are some really obvious conflicts like um you know Financial conflicts of interest but I think I think there's a whole deeper discussion about when you have sort of philosophical conflicts of interest with the person and that gets into another area which is peer review can be um blinded or or not blinded right it can be single-blinded where the reviewer knows who the author is but the author doesn't know who the feedback is from that tends to be very common I think that's probably the most common one I've seen they can be double-blinded where the reviewer doesn't know who it's being written by and vice versa and they can be completely open um but again the most common one that

[01:45:05] I've seen is is single-blinded um and and so you'll typically have three reviewers review something and they can either accept it outright reject it outright or make recommendations for revisions and again I I I think you'll see that as probably the most common thing where they say you know we're still interested in this paper but you know did you did you actually consider this hypothesis so sometimes the revisions are just repeat your analysis sometimes it's do another experiment uh that won't be the case in a clinical trial but if you're you know

[01:45:40] I I've had papers where that happened where I've done a series of experiments and I'd written it all up and I'd submitted and the reviewer came back and said well you really should have done this experiment as well um because this would have served as another control so you go and repeat that experiment of course when you're working in cell culture or something like that it's not it's not that onerous but and this process can go on several times uh but ultimately you know the editor makes a decision to accept that paper and publish it or reject it again and that and that's basically the process and and again you you're typically going to start at the at the top of the food chain so you're typically as a as an author you're going to try to get your paper published in the most prestigious Journal um I guess that's something we can talk about what determines what what determines the procedure of a journal um but you'll you'll you'll sort of keep going down the pecking order until you can get it into the right journal and and sometimes right out of the gate you just sort of know like hey this is this is a this is a publication that is really mechanistic and it's really going to be geared towards you know proceedings of the National

[01:46:43] Academy of Science versus something that has really got enormous clinical um implications and should go to Jama or during the Journal of Medicine so so there's there's a little bit of that that's going on as well so so to that end do all every study that's that's out there do they end up getting published

[01:47:01] No in fact many don't and this is I think this is a really big problem which is um you have this thing called publication bias so there's a very very famous example of this that you and I have spoken about which is the Minnesota heart study right and this is an example where a study was done um God it was it ran from what 1967 to 1973 if my memory serves me correctly so and it was looking at people who were in a residential care facility so they had complete control over what these patients ate and they were randomized to a diet of either normal saturated fat consumption or very low saturated fat consumption where the saturated fat was substituted with polyunsaturated fats and at the end of this seven year study of course the hypothesis being the group that was substituted saturated fat for plant saturated fat would have lower cholesterol levels and lower uh cardiovascular death rate and at the end of in 1973 when the study concluded they found that indeed the subjects who were given high amounts of polyunsaturated fats and low saturated fats did in fact have lower cholesterol levels but their rates of cardiovascular deaths were significantly greater and they didn't publish the study that study would remain unpublished until 1989 some 16 years later when asked why a 16-year delay in publishing that study the lead author who's I don't even remember who it was begins with an F I think it's Ivan Ivan

[01:48:38] France yeah France that's right yeah there's there's a senior and a junior yeah yeah uh he said the study didn't turn out the way we wanted it to so that's that's kind of an egregious example of publication bias in this case a negative study but you know I think there are a lot of studies that don't get published even if they're negative and that's a shame because when something doesn't work it is just as important as when it does work so it is unfortunate that not all studies get get published um because again just think about it this way if you want to go out and do an experiment and 10 people have done that experiment before you and it's always failed wouldn't it be great to know that would that impact your decision on whether or not you want to do the experiment a certain way or would you want to try something a little bit different so you can see very quickly this becomes problematic when when papers don't get published okay are there so you've got this massive problem publication bias do you know of any ways that that can combat this

[01:49:43] I think there are a lot of people working on this problem um and and I think um one of the important steps is pre-registration which we talked about at the outset right which is you force investigators to pre-register their experiments on clinicaltrials.gov so that and and that just that's not just here's my experiment it's here are my statistical methods here is my number of subjects here's my primary outcome here in my secondary outcomes Etc and that basically makes it a lot harder to say I'm not going to publish this when it comes out if it doesn't turn out the way I wanted it to and I think and I don't know if there are particular journals to participate in this but I I imagine that they could they could make it a a prerequisite that your your trial must be pre-registered in order to be published in our journal and if it's a journal you know worth publishing and it's probably not a bad idea correct um so there's there's both requirements of journals and there's also requirements of funding entities which say you're you can't we won't fund you unless the study is pre-registered um so uh you know I think that uh you know registered reports is a is a publishing format that an organization called the center for open science and I think that's the one founded by um

[01:50:59] Brian no sack is that his name I think that's right yeah yeah Brian would be a great guy to have on the uh on the podcast actually at some point so with registered reports um it it pulls back basically you submit your protocol almost like the pre-registration you submit that and at that point instead of after all the data is collected it's peer-reviewed and if it's peer-reviewed and accepted um based on you've got a high qual high quality protocol everything looks good then it's provisionally accepted for publication and so like you said like if it's a negative result and maybe that doesn't seem like it's worth publishing or a journal is not going to publish it they're basically making a decision if this is a positive or A negative trial whatever however it turns out your your protocol your plan looks really good and so we're going to accept it we're going to basically accept it provisionally provided that you don't you don't start cutting corners and go away from this plan that we accepted so you follow your plan and it's like however the cards fall it's already been accepted for publication yeah that's a that's a pretty novel concept actually um but again I think it's all in the spirit of how do we make sure that we get rid of publication bias positive result bias again going back to what we said a second ago you're far more likely to see something get published if it is a positive finding that if it's a negative finding although negative findings are just as important for the establishment of knowledge right let's use the ctep inhibitor yeah imagine no one had ever published the studies demonstrating that ctep Inhibitors were at best neutral at worst harmful again studies of that magnitude can't escape publication but think of all the bench research that can be going on or the small early phase one trials that can be going on or the pre-clinical stuff that's going on it's very easy to kind of um under report things that are that are negative yeah and one one other thing about the registers registered reports I was just thinking about which is gets to your power analysis like as an example let's say that your study is underpowered it would be great to have a a group of your peers you know more like you have to say tear your study apart but see if there's anything wrong with it and they say like your your study is powered to detect like a a 70 difference in in all cause mortality in some you know they might be pointing something out that basically it's saying your study's Dead on Arrival if you actually run it this way right because there's no way you're going to see an effect size greater than 30 percent and yet you're only powered to detect it if it's 70 which is crazy so either change the experimental design figure out a way to raise more money to do this study correctly um but yeah that's that that's a valuable tool okay and we we touched on this I think you talked about you know more reputable journals and that's one of the questions is I think people know that certain journals are more respected than others but is there like a is there a reason why in particular something's respected over another Journal yeah so so there's something called an impact factor and it it's it's usually it's something that changes each year um meaning it's usually evaluated on a per year basis so it's um it's the ratio between the number of citations total citations that point so citing something is referencing that paper so when you if you're writing a paper you would say well you know Kaplan wrote such and such and you you cite that paper it's a reference so great paper yeah great great paperback app so it's the it's the ratio between the total number of citations that come to all articles published by that journal by the total number of Articles published by the

[01:54:40] Journal over at previous period of time so it's typically done over a year so you would say okay uh you know 27 000 citations came into the journals published out of that article out of 10 000 articles so 27 000 divided by 10 000 would be 2.7 the impact factor would be 2.7 so to put this in context there's you know like 13 000 journals out there um most all you know 98 of them have an impact factor less than 10 uh about 95 of them have an impact factor less than five um and about half of them have an impact factor less than two uh just to give you kind of a sense of what impact factor looks like and by the way there's a the tail on that is very asymmetric so I mean the number of journals that have an impact factor of like point four point seven point eight I mean is is incredibly high so this is these are um uh if you look at the distribution of this is obviously a very long tail on the small end of this I've got a table here okay that I can pull up yeah yeah let's let's take a look at that because

[01:55:52] I think it's it's pretty cool to look at this actually got it so so you look at this table um you've highlighted the the journals that have more than 100 000 citations uh this is what year is it 2019. so you've got you know the New England Journal of Medicine uh which is kind of Staggering right nearly 350 000 citations and we could do the math but uh you can tell how many articles were published because if you divide 347 000 by that number you get 74.699 so that's the that's the impact factor for the New England Journal of

[01:56:27] Medicine um you know the Lancet 250 000 citations impact factor 60. so you can sort of see like these are the whatever top 28 journals by impact factor there's kind of an outlier right which is the uh the cancer journal for clinicians which has a staggering impact factor of 292 despite only having 40 000 citations so

[01:56:53] I that's a little bit of a skew I don't really consider um I don't really consider that to be sort of in the same league because it's it's it's basically the global Cancer Society statistic article and therefore it reports on tons of cancer statistics and therefore it doesn't really publish that much but it gets referenced so much because anytime someone is basically referencing a cancer statistic they're going to reference that so again I kind of put that in its own little category um but what you can see here is you know and the Same by the way notice that the who technical report Series has an impact factor of 59 but it's only cited like 3 500 times and so it's cited a lot for a very few number of Publications but again I think the ones that really matter here clinically the New England

[01:57:47] Journal of Medicine obviously Lancet Jamaica um are sort of your your your your huge clinical ones okay so one more question about reading a scientific paper so do you have a particular process when you read a read a paper it just print it out and start to finish start from the abstract and work your way through or do you have a particular process in general um yeah kind of um I mean I generally do read the abstract first um and and that kind of gives me a sense of am I interested in this paper that the title of the paper is usually not sufficient for me to know if I'm going to be interested but the abstract usually is my go no go on that so I could you know read 10 abstracts in a matter of minutes and decide you know how do I want to read three of these papers um the next decision I make is how familiar am I with this subject matter and if I'm not really familiar with it I will read the introduction section a lot of times I am relatively familiar with the subject matter so I'll just skip the introduction section altogether and I usually go straight to the methods section um and that gets into the details so okay you know this if this was an exercise study um and they did muscle biopsies I just want to really get right down to it so how many subjects were there how were they randomized what were the interventions when were the biopsies taken was there a crossover you know I just want to get into all that detail the next thing I do is I look at the results section but I start with the figures so I um I just kind of go right into read the look at the figure and read the legend and if the authors have done a good job it's almost Standalone at that point so figures and tables should in my opinion be Standalone so the legend should explain everything you need to know um and of course then reading the pros of the results section kind of adds a little bit more more color to that and then the last thing I do is I'll read the discussion section because I'll by this point have formulated my own thoughts on what the strengths and weaknesses of the studies are what questions remain Etc um but you know oftentimes the authors will have thought of things that I haven't thought of or they'll thought of things that I disagree with and and so

[02:00:09] I'll I'll kind of want to go through and do that so that's my general framework for it and you'll notice it's it's um quasi-linear but not entirely linear yeah I like that I'd like your example of figures I can't remember I might be misremembering but did Steve Rosenberg and this probably talks about like the importance of mentorship as well did he have advice as far as I think probably when you're when you're writing a paper how like your figures and what they should represent yeah that was that was our our process so when you finished an experiment the very first thing you did was you made the figures and legends for your uh your the figures and tables you made the those and the Legends first and that's what you would go in and present to him and you'd present that a journal club or not Journal Club at lab meeting rather um and you wouldn't really take pen to paper to write anything until you had that down so you you had to sort of know what are the relevant figures what are the relevant tables can I explain them very concisely in a legend and once you got that down the paper kind of wrote itself right the methods are really easy to write the results is easy to write and and sort of the last thing you would write would be the intro and the abstract um so and again I don't know that that's the that was just the way that I was taught to do it and I found that to be very productive okay I think we've run the list of questions we got through them all man this was uh there's a lot in here but um I'm glad I'm glad we did this we've been sort of um pushing this off for a couple of months now but but I think I'm glad we bit the bullet here me too great topic all right Bob I'll see you next time all right Peter thanks bye [Music]