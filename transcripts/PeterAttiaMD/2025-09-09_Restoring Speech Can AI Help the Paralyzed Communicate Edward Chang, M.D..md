# Restoring Speech: Can AI Help the Paralyzed Communicate? | Edward Chang, M.D.

**Channel:** Peter Attia MD
**Upload Date:** 2025-09-09
**URL:** https://www.youtube.com/watch?v=hza13Mm4NIA
**Duration:** 16 minutes

## Description

Get An Introductory Guide to Longevity and my weekly newsletter here (free): https://bit.ly/4mXcYg4
Watch the full episode:  https://youtu.be/y7LXTyUhSVM
Become a member to receive exclusive content: https://bit.ly/3JEumI3

This clip is from episode #363 ‒ A new frontier in neurosurgery: restoring brain function with brain-computer interfaces, advancing glioblastoma care, and new hope for devastating brain diseases | Edward Chang, M.D.

In this clip, they discuss:

- What Is BCI? – Brain-computer interface connects brain signals to a computer to create useful outputs
- Applications in Speech – Research explores decoding brain activity to restore communication for paralyzed patients
- ALS and Paralysis – Many patients keep cognition intact but lose the motor ability to speak
- AI Speech Decoding – Modern AI and speech synthesis tools decode brain activity into words
- Two Main Approaches – Noninvasive EEG on the scalp versus invasive electrodes placed directly on the cortex

--------
About:

The Peter Attia Drive is a deep-dive podcast focusing on maximizing longevity, and all that goes into that from physical to cognitive to emotional health. With over 90 million episodes downloaded, it features topics including exercise, nutritional biochemistry, cardiovascular disease, Alzheimer’s disease, cancer, mental health, and much more.

Peter Attia is the founder of Early Medical, a medical practice that applies the principles of Medicine 3.0 to patients with the goal of lengthening their lifespan and simultaneously improving their healthspan. 

Learn more: https://peterattiamd.com

Connect with Peter on:
Facebook: http://bit.ly/PeterAttiaMDFB
Twitter: http://bit.ly/PeterAttiaMDTW
Instagram: http://bit.ly/PeterAttiaMDIG

Subscribe to The Drive:
Apple Podcast: http://bit.ly/TheDriveApplePodcasts
Overcast: http://bit.ly/TheDriveOvercast
Spotify: http://bit.ly/TheDriveSpotify
Google Podcasts: http://bit.ly/TheDriveGoogle

Disclaimer: This podcast is for general informational purposes only and does not constitute the practice of medicine, nursing, or other professional healthcare services, including the giving of medical advice. No doctor-patient relationship is formed. The use of this information and the materials linked to this podcast is at the user's own risk. The content on this podcast is not intended to be a substitute for professional medical advice, diagnosis, or treatment. Users should not disregard or delay in obtaining medical advice for any medical condition they have, and they should seek the assistance of their healthcare professionals for any such conditions. I take conflicts of interest very seriously. For all of my disclosures and the companies I invest in or advise, please visit my website where I keep an up-to-date and active list of such companies. For a full list of our registered and unregistered trademarks, trade names, and service marks, please review our Terms of Use: https://peterattiamd.com/terms-of-use/

## AI Summary

Based on the provided transcript portion, I'll analyze what's covered:

1. **Executive Summary**:
The episode features Dr. Edward Chang discussing brain-computer interface (BCI) technology, specifically focusing on restoring speech in paralyzed patients. The discussion centers on a groundbreaking 2023 Nature paper involving a patient named Ann, who suffered a brain stem stroke in her 20s that left her quadriplegic and unable to speak. The research demonstrated successful implementation of ECOG (electrocorticography) technology to help her communicate.

2. **Key Medical/Scientific Points**:
- BCI involves recording brain signals and connecting them to a computer for analysis and transformation [00:00:00]
- Used 253 ECOG sensors covering an area approximately the size of a credit card [05:58]
- Initial accuracy was 50% on day one, improving to 95-100% within a week [10:58]
- The technology requires active attempt to speak, not just thinking about words [08:29]
- Patient had vertebral artery dissection causing brain stem stroke [04:39]

3. **Health Optimization Tips**:
Universal Recommendations:
- Early intervention may be more effective for speech recovery [13:32]
- Active rehabilitation can help strengthen oral facial muscles even years after injury [12:40]

4. **Notable Quotes**:
"BCIs are also going to be a way that we can do rehabilitation... it's a way that we have this direct readout of what the brain is trying to do." [13:02]

5. **References & Resources**:
- 2023 Nature paper on ECOG speech interface [03:45]
- NATO code words used as training vocabulary [09:39]

6. **Follow-up Questions**:
1. What is the maximum theoretical word-per-minute rate achievable with this technology?
2. How might this technology evolve for other neurological conditions?
3. What are the long-term implications for brain plasticity and rehabilitation?

Note: The transcript appears to be incomplete, so some sections (Supplements & Medications, Exercise & Movement, Nutrition & Diet, Biomarkers & Testing) cannot be addressed as they may have been discussed in other portions of the episode.

## Transcript

[00:00:00] So, let's go back to to to brain computer interface. What would be how would you explain this to somebody at a party if they said, "But what would you know that that that sounds pretty high-tech, but what is it?" Okay. Um, well, um, okay. So, let's just break apart the terms. Uh, brain refers to really any kind of thing that interfaces with the cortex or the deeper structures. Uh, the computer is a digital device on the outside. Um, a lot of people now call this BCI, brain computer interface for short. Um, it's a very messy term. um because it could mean a lot of different things but I think in a nutshell what it means is for most people a system that is recording from the brain whether it's non-invasive from the scalp or something that's fully invasive like within the brain itself and connecting those signals to a computer that analyzes the signal and then does something with it. In many cases of uh BCI research, the application is uh for example to remove a computer cursor or uh in the research that we've done is to replace speech words for someone who's severely paralyzed and unable to talk anymore.

[00:01:19] And so uh it's about interpreting brain signals and then using a computer to um interpret those signals and then um transform them into a form that's useful to us. So in that example you gave, you're describing a patient with aphasia who can't speak. Um let me be very specific about that. So a lot of the work that we've done is on people that have a severe form of paralysis and aphasia we typically I see refer to as someone who's got let's say a stroke. Yep.

[00:01:49] Um in the language centers of the brain. Um where we've focused recently is on patients that have a severe form paralysis like ALS. So there the problem is they have largely normal language but they can't get those can't get the motor signal out. Can't get the motor signal out to the vocal tract, the lips, the tongue, the jaw, the larynx. Those descending fibers are severely affected by ALS. They degenerate. Y and that's why people progressively become paralyzed and lose ability to speak.

[00:02:22] And an important part of that is that they lose ability to speak, but they still have full cognition. Yeah. And for that individual by attaching a computer to their brain, you're able to hopefully extract in written cursive text, you know, whatever across the computer screen what they're wishing to say. That's right.

[00:02:47] Okay. Let's talk about how that could possibly be done. Um, you mentioned earlier there are at least two broadly, you know, two broad ways to extract that information. A non-invasive way where presumably you're putting electrodes all over a head that's as well shorn as mine. Um, alternatively, a very invasive way where you actually remove the scalp and you lay these things on the cortex itself.

[00:03:14] Correct. Yeah. So the range would be EEG which is where sensors are placed on the scalp directly recording non-invasively. You can remove them at any time. And then the far other extreme is uh electrodes that are actually placed into the brain. So with ECOG, tell me how many words per minute you could capture from a patient with

[00:03:45] ALS. Okay. So um what we did in our clinical trial at UCSF, was this the 2023 paper? Yes. Okay. This is the nature paper. That's right. So we published a paper in 2023. Uh we worked with an a participant named an um she had a very severe brain stem stroke about uh 20 years ago. How old was an? She was in her 20s.

[00:04:13] Um it wasn't long after she had gotten married actually. Um just a couple months after her second daughter was born. She was playing volleyball with her friends collapsed. uh taken to the hospital. Uh she survived the injury. She woke up. Was it a vertebral artery? Yes, that's dissection. Yeah. Okay.

[00:04:39] Peter, you're they're very This is your memory from medical school is actually pretty good. You're This is impressive. Uh Ann survived this stroke. um she was left quadriplegic meaning she couldn't move her arms and legs uh but in addition to that she couldn't speak u because the nerves that come through the brain down through the brain stem and go to the cranial nerves which supply the vocal tract those were also directly affected and was the paralysis a result of her cerebellum also having infarks no no it was all brain stem related brain stem. Yeah, precisely the part that we call the ponds.

[00:05:22] Yeah. So, um Yeah. So, devastating. Devastating. So, it's for 20 years an is now in her 40s un in a wheelchair, unable to speak, right? And so, um what we did was we did a surgery where we implanted an array of 253 ECOG sensors. These are the sensors that are densely spaced. How many? 253. So, we're talking about something about the surface area of a credit card.

[00:05:58] And it's filled um with electrode sensors that are spaced about 3 mm apart. Each sensor is about a millimeter in diameter. And so basically you've got this credit card sized array that was placed on the part of our brain that processes words in particular the motor production of the words. The parts that control the lips, the jaw, the larynx, the tongue.

[00:06:26] Um areas that um were functionally disconnected from her vocal tract because of this stroke and the brain stem which connects the brain to those muscles. And so um um we did the surgery about uh three weeks later. We started our research sessions with her. We connected the uh cable. It's basically an HDMI cable that is attached to a head stage. The head stage transforms the analog signals from her brain. These are a small voltage. At the same time that we're doing all of this research, Peter

[00:07:12] AI is developing in parallel. All of these tools that we now are using every day, transcribing our voice right now into text, um, we use that technology. We can actually use those same technologies that generate voices called, uh, uh, speech synthesis. And so we've used a lot of the same tools, machine learning tools that are in modern day AIs and we're now applying them on the brain activity and trying to use them to not translate for example text and synthesized speech. But now the equation is different. It's translating from brain activity into synthesized speech.

[00:07:59] The input is not text. The input is the ecog activity across these 253 sensors for days. What we would do is have a sentence on a screen and we'd give her the start time and the end time and during that she would just look at the sentence. She's given a go and then try to say it. Nothing intelligible comes out. She may or may not be moving the lips jaw but just try to say and that turns out to be very important.

[00:08:29] Oh my god. like you can't just think about it, you can't just read it. You have to actually try to say it and that's what she did. Um so we started with a very simple vocabulary of about 27 words. Um the the words that we chose are the NATO code words. Alpha, Bravo, Charlie, Delta, Echo. Uh we did that because um we could measure basically the accuracy of the decoder that was analyzing those brain signals and translating them into those 26 different code words.

[00:09:11] And on the first day we were able to train the algorithm on you know a data set of like maybe about an hour and a half to get to about 50% accuracy. And does 50% accuracy mean she could get half of them right or any time you showed her one there was a 50% chance it would be correct both what you just said is in our sense identical.

[00:09:39] Okay. Like so she's looking and then trying there wasn't a bias towards a subset of them that she was always getting right and others that she was always getting there actually was a bias. Um yeah actually if I get into details yes some of them were more discriminable than others and was it based on the number of syllables could you okay yes actually it was based on that in uh some of the phonetic properties but one of the reasons why NATO code words for us was a really useful training training task for us is because NATO code words were developed in the first place by the military to improve communication accuracy. The reason why we actually use those code words is because sometimes if you just say a B, you know, like D Z, there's a lot of confusion. So that's why we actually use those code words. It increases the discriminability and intelligibility where a lot of those settings you just can't make those errors. For example, you know, pilots.

[00:10:39] Yep. You know, uh in the call numbers, for example. So um we use that because um it has high discriminability and on that first day um you know I think we got about 50% and you this is going straight to voice. No this was actually this is going straight this is going to text. This is straight to text. Yeah.

[00:10:58] So we're just trying to figure out could we um decode which word it was and it was displayed in text. Um so that's pretty amazing. That's the first day. That's the first day. Yeah. And then over the next I would say about six days uh the performance just got better and better and then by about you know like a week into this she was up into the 95 100% range. Um so that was um unexpected.

[00:11:30] Um it was incredible to see the performance increase you know so quickly but that did take a full week to do that. Did you get a sense from an as to how her level of fatigue with this progressed? Um, in other words, what becomes the bottleneck? Is it does it get easier and easier for her to go through this talking motion as she practices more? Is it just like any other muscle that we think of that has sort of atrophied and now she's sort of getting her talking back in shape?

[00:12:05] It is a bit of that. And um we're trying to make that easier over time. I think in the beginning days we were trying everything to get it to work and um a lot of it again has to do with this valitional intent to speak. That turns out to be the most critical thing. Peter, one of the things that I thought was really interesting also was we were doing so much um decoding through these tasks that over time, actually a couple months into this,

[00:12:40] Ann had reported to us actually the strength of her oral facial muscles, her jaw, lip, tongue, they were actually getting stronger through this constant therapy, constant rehabilitation. And so I think right now everything is about just decoding the brain activity, you know, to an artificial digital thing.

[00:13:02] But I do think that in the future BCIs are also going to be a way that we can do rehabilitation, right? Like it's a way that we have this direct readout of what the brain is trying to do. you can essentially build a prosthetic that helps people speak, but in the process, someone who hasn't spoken for a while will regain some of that natural strength over time. So, that's a new indication that we're thinking about in the future, how to use this technology actually to augment and accelerate rehabilitation.

[00:13:32] If Anne had that stroke today, how different, if at all, would this process look if you were working with a person who hadn't spent 20 years or 18 years without speaking? Um, there's no question that I think it would work faster. There's less to learn. You know, for her not speaking for 18 years basically meant that she basically had to relearn how to speak and we had to keep up with her relearning. Her brain was probably reorganizing, relearning actually some of those fundamental things and she could see the feedback of essentially whether or not she what she was trying to say was right or wrong.

[00:14:22] Um, and it was very intense work. So, we're trying to make that easier over time. Um but I think certainly the more preserved, the more recent you know that activity is, yeah, uh those memories, the synapses we talked about earlier, the more stable, the more functional they are, the easier it is to actually decode them.

[00:14:46] So what will be the ceiling for the current technology? How many words per minute and at what resolution or accuracy do you think the current technology with because this was ecog in her case correct? Right. Um where do you think it's going to go? Where do you think it where will this asmtote? I'm Peter Atia. This podcast relies exclusively on premium subscribers for support which allows us to provide all our content without taking a single penny from advertisers. I believe this keeps my content honest, making it a trusted resource for listeners like you.

[00:15:25] As a premium member, you'll get immediate access to our entire back catalog of AMA episodes and all future AMA episodes. You'll get longevity focused premium articles packed with actionable insights. You'll get unrivaled show notes for each and every episode of the drive. Every topic, every study, every resource from each episode carefully curated for you. You'll get quarterly podcast summaries where you'll learn my biggest personal takeaways from the previous 90 days of expert guest episodes and much more. This journey doesn't have to be navigated alone. We can take these steps towards a better, longer life together. Become a premium member today at peterati md.com/subscribe to join me in a shared commitment to a healthier future. [Music]

[00:16:26] [Music]